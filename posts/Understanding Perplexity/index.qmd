---
title: "Understanding Perplexity"
subtitle: "A New Perspective on Model Uncertainty"
author: "Swayam Singh"
date: "2024-10-10"
categories: [LLM]
format:
  html:
    code-fold: false
jupyter: python3
toc: true
highlight-style: pygments
execute: 
  freeze: auto
image: meme.jpg
social:
  - image: meme.jpg
    description: "Meme representing Perplexity"
---

# Understanding Perplexity: A New Perspective on Model Uncertainty
<hr>

![](meme.jpg){fig-alt="Meme representing perplexity" fig-align="center" width=70%}

<hr>
Recently, I was diving into Chapter 5 (Pretraining) of the insightful book *"[Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)"* by [Sebastian Raschka](https://x.com/rasbt?lang=en). While exploring the intricacies of language models, I stumbled upon an intriguing interpretation of **perplexity**. The author noted:


> "Perplexity is often considered more interpretable than the raw loss value because it signifies the effective vocabulary size about which the model is uncertain at each step."

*In simple words*,
If for some model the perplexity comes out to be $N$ then it means that the model is $N$ tokens uncertain about the correct next-token, it is considering all the $N$ tokens as the potential candidate for the output token.

This statement resonated with me, as I had always viewed perplexity as just a performance metric. I began to wonder: <strong><i><u>can we mathematically derive this interpretation? Does the underlying math support this idea?</u></i></strong>

Let’s delve into the equations and explore how perplexity relates to the model’s uncertainty about the next token in a sequence.

## Cross-Entropy Loss: A Quick Recap

In language modeling, **cross-entropy loss** is a critical metric that helps us evaluate how well a model predicts the next token in a sequence. For a sequence of tokens $x = (x_1, x_2, ..., x_T)$, the cross-entropy loss is calculated as:

$$
\mathcal{L} = - \frac{1}{T} \sum_{t=1}^{T} \log P(x_t | \mathbf{x}_{<t})
$$

where:
<li> $T$ is the total number of tokens in the sequence. </li>
<li> $P(x_t | \mathbf{x}_{<t})$ is the predicted probability of the actual token $x_t$ given the preceding context $\mathbf{x}_{<t}$. </li>
<br>
This formulation averages the negative log-likelihood across all tokens, providing a measure of how well the model's predictions align with the true tokens.

## Defining Perplexity

**Perplexity** serves as a complementary metric to cross-entropy loss and is defined as the exponentiation of the loss:

$$
\text{Perplexity} = \exp(\mathcal{L})
$$

This formulation provides a more interpretable value, as it represents the effective number of choices the model considers when predicting the next token. A lower perplexity indicates higher confidence in predictions, while a higher perplexity signifies greater uncertainty.

::: {.callout-note}
Before going into maths, lets understand one thing

Intuitively, for a completely uncertain model, selection for some next-token can be any from the whole vocabulary with each token having same probability of being the next token
:::

## Analyzing the Uniform Distribution Case

To understand the interpretation of perplexity in terms of effective vocabulary size, let’s consider an extreme case where the model is **completely uncertain** about the next token. In this scenario, the model assigns equal probability to every token in the vocabulary of size $V$. Thus, the probability of each token can be expressed as:

$$
P(x_t | \mathbf{x}_{<t}) = \frac{1}{V}
$$

Now, substituting this uniform probability into the cross-entropy loss equation, we get:

$$
\mathcal{L} = - \log P(x_t | \mathbf{x}_{<t}) = - \log \frac{1}{V} = \log V
$$

Here, $-\log P(x_t | \mathbf{x}_{<t})$ reflects the loss incurred for each token when the model is entirely uncertain.

## Connecting Loss and Perplexity

Next, we can use the **perplexity** formula to analyze this situation:

$$
\text{Perplexity} = \exp(\mathcal{L}) = \exp(\log V) = V
$$

This result reveals a fascinating insight: when the model is completely uncertain, the perplexity is exactly equal to the size of the vocabulary $V$. 

### Effective Vocabulary Size

Now, what does this mean in terms of interpretation? When the perplexity equals $V$, it indicates that the model is effectively considering all $V$ tokens as potential candidates for the next token, reflecting a state of maximum uncertainty.

On the other hand, if the model has a lower perplexity, say 100, it means that the model behaves as if it is uncertain only among 100 tokens. This aligns perfectly with the statement from Raschka's book: ***perplexity signifies the effective vocabulary size about which the model is uncertain at each step.***