{
  "hash": "1b65ff37b8a97e30455ff8e5f86cd85c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Understanding Perplexity\"\nsubtitle: \"A New Perspective on Model Uncertainty\"\nauthor: \"Swayam Singh\"\ndate: \"2024-10-10\"\ncategories: [LLM]\nformat:\n  html:\n    code-fold: false\njupyter: python3\ntoc: true\nhighlight-style: pygments\nexecute: \n  freeze: auto\nimage: meme.jpg\nsocial:\n  - image: meme.jpg\n    description: \"Meme representing Perplexity\"\n---\n\n\n\n\n# Understanding Perplexity: A New Perspective on Model Uncertainty\n<hr>\n\n![](meme.jpg){fig-alt=\"Meme representing perplexity\" fig-align=\"center\" width=70%}\n\n<hr>\nRecently, I was diving into Chapter 5 (Pretraining) of the insightful book *\"[Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)\"* by [Sebastian Raschka](https://x.com/rasbt?lang=en). While exploring the intricacies of language models, I stumbled upon an intriguing interpretation of **perplexity**. The author noted:\n\n\n> \"Perplexity is often considered more interpretable than the raw loss value because it signifies the effective vocabulary size about which the model is uncertain at each step.\"\n\n*In simple words*,\nIf for some model the perplexity comes out to be $N$ then it means that the model is $N$ tokens uncertain about the correct next-token, it is considering all the $N$ tokens as the potential candidate for the output token.\n\nThis statement resonated with me, as I had always viewed perplexity as just a performance metric. I began to wonder: <strong><i><u>can we mathematically derive this interpretation? Does the underlying math support this idea?</u></i></strong>\n\nLet’s delve into the equations and explore how perplexity relates to the model’s uncertainty about the next token in a sequence.\n\n## Cross-Entropy Loss: A Quick Recap\n\nIn language modeling, **cross-entropy loss** is a critical metric that helps us evaluate how well a model predicts the next token in a sequence. For a sequence of tokens $x = (x_1, x_2, ..., x_T)$, the cross-entropy loss is calculated as:\n\n$$\n\\mathcal{L} = - \\frac{1}{T} \\sum_{t=1}^{T} \\log P(x_t | \\mathbf{x}_{<t})\n$$\n\nwhere:\n<li> $T$ is the total number of tokens in the sequence. </li>\n<li> $P(x_t | \\mathbf{x}_{<t})$ is the predicted probability of the actual token $x_t$ given the preceding context $\\mathbf{x}_{<t}$. </li>\n<br>\nThis formulation averages the negative log-likelihood across all tokens, providing a measure of how well the model's predictions align with the true tokens.\n\n## Defining Perplexity\n\n**Perplexity** serves as a complementary metric to cross-entropy loss and is defined as the exponentiation of the loss:\n\n$$\n\\text{Perplexity} = \\exp(\\mathcal{L})\n$$\n\nThis formulation provides a more interpretable value, as it represents the effective number of choices the model considers when predicting the next token. A lower perplexity indicates higher confidence in predictions, while a higher perplexity signifies greater uncertainty.\n\n::: {.callout-note}\nBefore going into maths, lets understand one thing\n\nIntuitively, for a completely uncertain model, selection for some next-token can be any from the whole vocabulary with each token having same probability of being the next token\n:::\n\n## Analyzing the Uniform Distribution Case\n\nTo understand the interpretation of perplexity in terms of effective vocabulary size, let’s consider an extreme case where the model is **completely uncertain** about the next token. In this scenario, the model assigns equal probability to every token in the vocabulary of size $V$. Thus, the probability of each token can be expressed as:\n\n$$\nP(x_t | \\mathbf{x}_{<t}) = \\frac{1}{V}\n$$\n\nNow, substituting this uniform probability into the cross-entropy loss equation, we get:\n\n$$\n\\mathcal{L} = - \\log P(x_t | \\mathbf{x}_{<t}) = - \\log \\frac{1}{V} = \\log V\n$$\n\nHere, $-\\log P(x_t | \\mathbf{x}_{<t})$ reflects the loss incurred for each token when the model is entirely uncertain.\n\n## Connecting Loss and Perplexity\n\nNext, we can use the **perplexity** formula to analyze this situation:\n\n$$\n\\text{Perplexity} = \\exp(\\mathcal{L}) = \\exp(\\log V) = V\n$$\n\nThis result reveals a fascinating insight: when the model is completely uncertain, the perplexity is exactly equal to the size of the vocabulary $V$. \n\n### Effective Vocabulary Size\n\nNow, what does this mean in terms of interpretation? When the perplexity equals $V$, it indicates that the model is effectively considering all $V$ tokens as potential candidates for the next token, reflecting a state of maximum uncertainty.\n\nOn the other hand, if the model has a lower perplexity, say 100, it means that the model behaves as if it is uncertain only among 100 tokens. This aligns perfectly with the statement from Raschka's book: ***perplexity signifies the effective vocabulary size about which the model is uncertain at each step.***\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}