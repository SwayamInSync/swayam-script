[
  {
    "objectID": "pages/A Hitchhiker's Guide to LLVM/chapter-1.html",
    "href": "pages/A Hitchhiker's Guide to LLVM/chapter-1.html",
    "title": "Chapter 1: Introduction to LLVM",
    "section": "",
    "text": "Chapter 1: Introduction to LLVM"
  },
  {
    "objectID": "posts/C++/index.html#item-1-view-c-as-a-federation-of-languages",
    "href": "posts/C++/index.html#item-1-view-c-as-a-federation-of-languages",
    "title": "[WIP] Effective C++ (Complete series)",
    "section": "Item-1: View C++ as a federation of languages",
    "text": "Item-1: View C++ as a federation of languages\nC++ encapsulates multiple programming paradigms Object Oriented, Template Metaprogramming, Functional, Generic. It is more of a federation of languages."
  },
  {
    "objectID": "posts/C++/index.html#item-2-prefer-const-enum-and-inline-to-define",
    "href": "posts/C++/index.html#item-2-prefer-const-enum-and-inline-to-define",
    "title": "[WIP] Effective C++ (Complete series)",
    "section": "Item-2: Prefer const enum and inline to #define",
    "text": "Item-2: Prefer const enum and inline to #define\n\nPrefer compiler over preprocessor\n\n\nDon’t\nConsider the following piece of code\n#define ASPECT_RATIO 1.653\nThe issue is that the symbolic name “ASPECT RATIO” is never visible to compiler as it get replaced by the preprocessor everywhere within the code making it does not being registered on symbol table. This makes it interpret the errors during compilation as it’ll refer to 1.653 rather than ASPECT RATIO, also makes hard in symbolic debugging as there is no corresponding symbol.\n\n\nDo\nconst double ASPECT_RATIO = 1.653;\nThis can not only register in symbol table but also allows compiler to dispatch optimisations.\n2 Special cases to consider when using const in place of #define\n\nConstant pointers\nIt is advised to do a const pointer pointing to a const object i.e. \nconst char *const name = \"Swayam\";\n// OR\nconst std::string name(\"Swayam\");\nClass-Specific constants\nTo ensure the scope of constant to a class, we need to make it a member and also to make sure there are no duplicate copies, make it static\nclass GamePlayer{\nprivate:\n    static const int NUM_TURNS = 5; // constant declaration\n    int score[NUM_TURNS];\n}\nAn important point here is that the above is constant declaration (not definition). C++ requires programmer to provide the definition for anything we use, but class-specific constants that are static and of integral type (int, char, bool) are exception.\nAs long as we don’t use their address, we can declare them and use them without providing a definition\nThe definition can be provided as follows (must need to be at the namespace scope level)\nconst int GamePlayer::NUM_TURNS;\nNot providing any value as it is being already initialized at declaration. For any other (non-integral data type) initialization at declaration time can give error (Can use constexpr to achieve that)\n#include&lt;iostream&gt;\nclass A\n{\n    private:\n    static const double f;\n  // constexpr static const double f = 10.9; // this also works\n    public:\n    void print()\n    {\n        std::cout &lt;&lt; this-&gt;f &lt;&lt; std::endl;\n    }\n\n};\nconst double A::f = 10.9; // if not then get linking error that undefined reference\nint main()\n{\n    A a = A();\n    a.print();\n    return 0;\n} \nNote that cannot use #define for class scoped constants, as those macros are entire lifetime unless manually being #undef nor they agree to encapsulation like private or public\n\nThe enum trick\nenum values are compile-time constants built into the type system, no storage needed. Consider the following code\n#include&lt;iostream&gt;\nclass A\n{\n    private:\n    static const int NUM;\n    int scores[NUM];\n};\nconst int A::NUM = 10\nHere NUM needs to be known at compile time to create the array of NUM size but that is defined later in the scope. We can use the enum hack for this\n#include&lt;iostream&gt;\nclass A\n{\n    private:\n    enum {NUM = 10};\n    int scores[NUM];\n};\nThis is worth knowing as\n\nIt is more like #define than const , i.e. it is legal to take the address of a const but it is not legal to take the address of an #define macro as well as same for enum\nGood compilers usually don’t allocate storage for compile time constants, unless user is access the address somewhere in code\nOther reason for worth knowing is that lots of code use this, so need to be aware.\n\n\nAnother major issue using #define directive is using it to implement macros that look like functions but that don’t incur the overhead of a function call.\n// call f with the maximum of a and b\n#define CALL_WITH_MAX(a, b) f((a) &gt; (b) ? (a) : (b))\nThis is very painful to think even, you have to remember to parenthesize all the arguments in the macro body. I prefer it doing as\ntemplate &lt;typename T&gt;\ninline void CALL_WITH_MAX(const T &a ,const T &b){\n    f (a &gt; b ? a : b)\n}"
  },
  {
    "objectID": "posts/C++/index.html#item-3-use-const-whenever-possible",
    "href": "posts/C++/index.html#item-3-use-const-whenever-possible",
    "title": "[WIP] Effective C++ (Complete series)",
    "section": "Item-3: Use const whenever possible",
    "text": "Item-3: Use const whenever possible\n\nconst and pointers\nConsider the following code\nconst int *p; // non-const pointer to const int data\nint *const p; // const pointer to non-const int data\nint const *p; // non-const pointer to const int data\nconst int *const p; // const pointer to const int data\nRule of thumb: const applies to whatever the immediate left to it, if nothing then whatever is immediate right to it.\n\n\nFunction declarations\nIt is a good habit to keep the const constraints on the function declaration to keep the consistency with the client. Also returning a const from a function might not make sense but sometimes it can be a good habit\nclass Rational {...};\n\nconst Rational operator* (const Rational &lhs, const Rational &rhs);\n\n// this can prevent the doings like\n// (a*b) = c although sometimes this is what a user wants but unlikely from a user-defined type\n\n\nconst Member functions\nThe purpose of const member functions is to operate on the const objects (they can also call by the non-const objects) but gurantees to not modify the object’s properties. The 2 variants can be overloaded to ensure the seprate functionalities\nclass MyVector {\nprivate:\n    int* data;\n    size_t size;\n    \npublic:\n    MyVector(size_t s) : size(s) {\n        data = new int[s];\n    }\n    \n    // Non-const version - returns modifiable reference\n    int& operator[](size_t index) {\n        std::cout &lt;&lt; \"Non-const version called\\n\";\n        return data[index];\n    }\n    \n    // Const version - returns read-only reference  \n    const int& operator[](size_t index) const {\n        std::cout &lt;&lt; \"Const version called\\n\";\n        return data[index];\n    }\n    \n    ~MyVector() { delete[] data; }\n};\n\nint main() {\n    MyVector vec(5);           // non-const object\n    const MyVector constVec(5); // const object\n    \n    vec[0] = 10;      // ✅ Calls non-const version, returns int&\n    int x = vec[0];   // ✅ Calls non-const version (but we can't modify)\n    \n    int y = constVec[0]; // ✅ Calls const version, returns const int&\n    // constVec[0] = 20;    // ❌ ILLEGAL - const version returns const int&\n}\nThere are 2 notions of defining constness\n\nbitwise const: No member variables are modified (this is what c++ follows)\nclass CTextBlock {\npublic:\n  char& operator[](std::size_t position) const   \n  { return pText[position]; }                    \n\nprivate:\n  char *pText;  // The POINTER itself is not modified\n};\n\nint main() {\n  const CTextBlock ctb(\"Hello\");\n\n  ctb[0] = 'J';  // ✅ Compiles! But we just modified a const object!\n  // Now the \"const\" object \"ctb\" contains \"Jello\" instead of \"Hello\"\n}\nlogical constness The object’s observable state doesn’t change (this is what violated above). There can be cases when a method cannot be bitwise const but logically it can make sense\nclass CTextBlock {\npublic:\n\n  ...\n\n  std::size_t length() const;\n\nprivate:\n  char *pText;\n  std::size_t textLength;            // last calculated length of textblock\n  bool lengthIsValid;                // whether length is currently valid\n};\n\nstd::size_t CTextBlock::length() const\n{\n  if (!lengthIsValid) {\n    textLength = std::strlen(pText);  // error! can't assign to textLength\n    lengthIsValid = true;             // and lengthIsValid in a const\n  }                                   // member function\n\n  return textLength;\n}\nIt seems fine “logically” even for a const object but compiler won’t agree as it violates the bitwise constness. Solution is in the next subsection\n\n\nmutable\nmutable keyword frees the non-static data members from the constraints of bitwise constness.\nclass CTextBlock {\npublic:\n\n  ...\n\n  std::size_t length() const;\n\nprivate:\n  char *pText;\n  mutable std::size_t textLength; // it can vary\n  mutable bool lengthIsValid; // this too, even in const member functions\n};\n\nstd::size_t CTextBlock::length() const\n{\n  if (!lengthIsValid) {\n    textLength = std::strlen(pText); // now fine \n    lengthIsValid = true; // also fine\n  }\n\n  return textLength;\n}\n\n\nCasting away the constness\nConsider one more scenario, mutable is actually good to solve some certain mutability within const methods, but still usually we have 2 overloaded function of operator[] one returning the direct alias and other returning an const alias for const objects. Situation can comes in that both of these oeprators perform some more work like, reading data, manipulting it, some conditional checks, some logging, etc. This can lead to redundant code duplicate as both performing same thing. One solution is to one or more private methods and call them but still the calls are duplicated, so the idea what if we implement one operator[] and use it twice\n\nIt is a bad idea, but sometimes can be taken as a pinch of salt\n\nSo one might try\nchar& operator[](std::size_t position) {\n    return (*this)[position];  // ❌ INFINITE RECURSION! \n                              // Calls itself, not the const version\n}\nHere *this is still the same non-const object hence everytime it is going to call itself, following is the step by step understanding of right method using const_cast\n// step 0: Have the const version implemented\n// step 1: cast the non-const object (*this) to const\nstatic_cast&lt;const TextBlock&&gt;(*this);\n\n// step 2: now call the [] operator (should call the const version)\nstatic_cast&lt;const TextBlock&&gt;(*this)[position]; // this returns `const char&`\n\n// step 3: Remove const from returned value\nconst_cast&lt;char &&gt;(\n  static_cast&lt;const TextBlock&&gt;(*this)[position]\n);\n\n============================================================================================\n// In end it should be like\n\nconst char & operator[](std::size_t position) const {\n  .... // preprocessing\n  return text[position];\n}\n\nchar & operator[](std::size_t position) {\n  return const_cast&lt;char &&gt;(\n    static_cast&lt;const TextBlock&&gt;(*this)[position]\n  );\n}\n\n\n\n\n\n\nNote\n\n\n\nNote-1: if *this is already a const object then you CANNOT do static_cast&lt;TextBlock &&gt;(*this); i.e. you cannot remove the constness, use const_cast for that\nNote-2: having the non-const implemented and calling the const will get compiled but again, there are chances that implemented non-const can modify the properties, so always call const from a non-const"
  },
  {
    "objectID": "posts/C++/index.html#item-4-make-sure-that-objects-are-initialized-before-theyre-used",
    "href": "posts/C++/index.html#item-4-make-sure-that-objects-are-initialized-before-theyre-used",
    "title": "[WIP] Effective C++ (Complete series)",
    "section": "Item-4: Make sure that objects are initialized before they’re used",
    "text": "Item-4: Make sure that objects are initialized before they’re used\n\n\n\n\n\n\nTip\n\n\n\nIts my own experience in working with different kernels and Operating Systems, please initialize after allocation, else a heisenbug will be waiting for you\n\n\n\n\n\n\n\n\nNote\n\n\n\nReading uninitialized values is an undefined behaviour\n\n\nSo there are rules that decide when object initialization is guranteed to take place and when not, but those rules are complex. So just initialize them if you are going to use them.\n\nDifference between assignment and initialization\nConsider the following code\nclass PhoneNumber {...};\n\nclass ABEntry {\npublic:\n  ABEntry(const std::string & name, const std::string &address, const std::list&lt;PhoneNumber&gt; &phones);\nprivate:\n  std::string theName;\n  std::string theAddress;\n  std::list&lt;PhoneNumber&gt; thePhones;\n  int numTimesConsulted;\n};\n\nABEntry::ABEntry(const std::string & name, const std::string &address, const std::list&lt;PhoneNumber&gt; &phones) {\n  theName = name; // all these are not initializations, they are assignments\n  theAddress = address;\n  thePhones = phones;\n  numTimesConsulted = 0;\n}\nIt may work with the values we expect but THIS IS NOT INITIALIZATION, IT IS ASSIGNMENT.\n\n\n\n\n\n\nNote\n\n\n\nThe rule is that, data members of an object are initialized before the body of the constructor is entered\n(This isn’t true for built-in types though)\n\n\nA better way is to use Member Initialization List instead of assignments, remember the initialization order must follow the declaration order of members\nABEntry::ABEntry(const std::string& name, const std::string& address,\n                 const std::list&lt;PhoneNumber&gt;& phones)\n: theName(name),\n  theAddress(address),                  // these are now all initializations\n  thePhones(phones),\n  numTimesConsulted(0)\n{}                                      // the ctor body is now empty\nEarlier with constructor assignment, the compiler will call the default constructor and then use copy assignment operator to override the values. With Member Initialization list, all the members are initialized using the copy-constructor of corresponding objects.\nWe can also use the same for default constructor\nABEntry::ABEntry()\n:theName(),                         // call theName's default ctor;\ntheAddress(),                      // do the same for theAddress;\nthePhones(),                       // and for thePhones;\nnumTimesConsulted(0)               // but explicitly initialize\n{}                                  // numTimesConsulted to zero\n\n\n\n\n\n\nNote\n\n\n\nObjects like const and references must be initialized, they can’t be assigned, hence here the use of member initialization list method is a must\n\n\n\n\nOrder of initialization of non-local static objects defined in different translation unit\n\nA static object is one that exists from the time it’s constructed until the end of the program\nStack and heap-based objects are thus excluded. Included are global objects, objects defined at namespace scope, objects declared static inside classes, objects declared static inside functions, and objects declared static at file scope\nStatic objects inside functions are known as local static objects (because they’re local to a function) and the other kinds of static objects are known as non-local static objects\nStatic objects are automatically destroyed when the program exits i.e destructor is called on main function exit\nA translation unit is basically, source file + all of its #include files\n\nSo the issue is, lets say for initialization our logic depends on the initialization of some other “non-local static object in another translation unit” then there is no way to gurantee that it is being already initialized and good to use. This is actually undefined.\n// File: logger.cpp\nclass Logger {\npublic:\n    void log(const std::string& msg) { /* ... */ }\n};\n\nLogger globalLogger;  // Non-local static object\n\n// File: database.cpp  \n#include \"logger.h\"\nextern Logger globalLogger;\n\nclass Database {\npublic:\n    Database() {\n        globalLogger.log(\"Database initialized\");  // ❌ DANGER!\n        // What if globalLogger isn't initialized yet?\n    }\n};\n\nDatabase globalDB;  // Another non-local static object\nSolution? Function-Local Static Objects wrap those non-static into a function and convert them into a local-static objects, that way refer to that function and it is being guranteed to be initialized\n// File: logger.cpp\nclass Logger {\npublic:\n    void log(const std::string& msg) { /* ... */ }\n};\n\n// Replace global static with function returning local static\nLogger& getLogger() {\n    static Logger instance;  // Local static - initialized on first call\n    return instance;\n}\n\n// File: database.cpp\nLogger& getLogger();  // Declaration\n\nclass Database {\npublic:\n    Database() {\n        getLogger().log(\"Database initialized\");  // ✅ SAFE!\n        // getLogger() ensures Logger is initialized before we use it\n    }\n};\n\nDatabase& getDatabase() {\n    static Database instance;  // Local static\n    return instance;\n}\n\n\n\n\n\n\nNote\n\n\n\nC++11 and further guarantees that Static local initialization is thread-safe\nLogger& getLogger() {\n    static Logger instance;  // ✅ Thread-safe since C++11!\n    return instance;\n}"
  },
  {
    "objectID": "posts/A Hitchhiker's Guide to LLVM/index.html#building-clang-trust-me-youll-need-it",
    "href": "posts/A Hitchhiker's Guide to LLVM/index.html#building-clang-trust-me-youll-need-it",
    "title": "[WIP] A Hitchhiker’s Guide to LLVM",
    "section": "Building clang (trust me you’ll need it)",
    "text": "Building clang (trust me you’ll need it)\nClang is a llvm based compiler driver which provide frontend and invokes right tools for the llvm backend to compile the C-like languages.\ngit clone https://github.com/llvm/llvm-project.git\ncmake -DLLVM_ENABLE_PROJECTS=clang -GNinja -DCMAKE_BUILD_TYPE=Release llvm\nninja clang \n\n\n\n\n\n\nTip\n\n\n\nHow I know the tags you may ask? checkout the llvm-project/llvm/CMakeLists.txt file and search for LLVM_ALL_PROJECTS and read the comments above it.\nOR the instructions are also available here: https://clang.llvm.org/get_started.html"
  },
  {
    "objectID": "posts/A Hitchhiker's Guide to LLVM/index.html#building-llvm-yeah-the-dragon-itself",
    "href": "posts/A Hitchhiker's Guide to LLVM/index.html#building-llvm-yeah-the-dragon-itself",
    "title": "[WIP] A Hitchhiker’s Guide to LLVM",
    "section": "Building LLVM (yeah the dragon itself)",
    "text": "Building LLVM (yeah the dragon itself)\nmkdir build\ncd build\ncmake -G Ninja \\\n  -DCMAKE_BUILD_TYPE=Debug \\\n  -DLLVM_ENABLE_PROJECTS=\"all\" \\\n  -DLLVM_OPTIMIZED_TABLEGEN=1 \\\n  ../llvm\nninja # this will build all the targets\nNotably some major targets generated inside you’ll find inside build/bin are:\n\nopt: Driver for performing optimizations on LLVM IR i.e. LLVM IR =&gt; Optimized LLVM IR\nllc: Driver for transforming LLVM IR to assembly or object file\nllvm-mc Driver to interact with assembled and disassembled object\ncheck Driver to run test\n\nLet’s quickly run the check target with ninja and it’ll automatically run the test suite\nninja check\n\n# or specific target\nninja check-&lt;target-name&gt;\n\n# can print all the targets available by\nninja help\nLLVM also has llvm-lit tool that runs the specified tests\n./bin/llvm-lit test/CodeGen/RISCV/GlobalISel\n\n# Output\n-- Testing: 403 tests, 96 workers --\nPASS: LLVM :: CodeGen/RISCV/GlobalISel/regbankselect/fp-arith-f16.mir (1 of 403)\nPASS: LLVM :: CodeGen/RISCV/GlobalISel/legalizer/legalize-bswap-rv64.mir (2 of 403)\nPASS: LLVM :: CodeGen/RISCV/GlobalISel/irtranslator/calls.ll (3 of 403)\n[...]\nThere is also a tool inside LLVM known as FileCheck this basically governs the verification of test outputs. I won’t be going in depth but take it as you can write the expected outputs within comments inside the IR or any file and the lit when running test invoke FileCheck for verification, here is a small example\n; RUN: opt &lt; %s -passes=mem2reg | FileCheck %s\n\ndefine i32 @example() {\n  ; CHECK-LABEL: @example(\n  ; CHECK-NOT: alloca\n  ; CHECK: ret i32 42\n\n  %x = alloca i32\n  store i32 42, ptr %x\n  %result = load i32, ptr %x\n  ret i32 %result\n}\nIn this case FileCheck will verify that the optimized IR has a label “@example” and optimize the current IR by removing the “alloca” instruction and directly returning 42 as int32.\nWe can also verify this by on our own quickly. Create a test.ll file with the exact same content as shown above and since we already build the LLVM so we have the required tools to test this, run the following commands:\n# This will output the optimized IR (you can verify the pattern by yourself)\n./bin/opt -S &lt; ../../tweaks/test.ll -passes=mem2reg\n# Here we are using the generated optimized IR as stdin to the FileCheck\n./bin/opt -S &lt; ../../tweaks/test.ll -passes=mem2reg | ./bin/FileCheck ../../tweaks/test.ll  # no output mean success\n\n\n\n\n\n\nNote\n\n\n\n\n./bin/opt &lt; ../../tweaks/test.ll -passes=mem2reg: will generate the bytecode by default, use -S to get the textual representation\npasses=mem2reg is just telling opt driver what optimization pass to apply\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTools like lit and FileCheck are build by LLVM folks but they are very general to use for other projects and langauge as well, semi-colon ; comments are specific to LLVM but these tools can work with any language and its comment styles\nRead more about them and usage:\n\nlit: https://llvm.org/docs/CommandGuide/lit.html\nFileCheck: https://llvm.org/docs/CommandGuide/FileCheck.html\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou should be thinking of a question at the moment (atleast I had):\nThe optimization can reorder the instruction, how these checks would be valid in that case? well short answer, go to the docs and read about CHECK-DAG\n\n\nLLVM tests are located inside the build directory as: - unittests: typical test written using gtest suite - tests: they are the lit tests, we already saw the example of a typical lit test in llvm above"
  },
  {
    "objectID": "posts/A Hitchhiker's Guide to LLVM/index.html#modules",
    "href": "posts/A Hitchhiker's Guide to LLVM/index.html#modules",
    "title": "[WIP] A Hitchhiker’s Guide to LLVM",
    "section": "Modules:",
    "text": "Modules:\nA module is essentially the container for everything the compiler is working on at a given time. Think of it as:\nInput File (e.g., main.c) → LLVM Module\nThe module contains:\n\nAll function definitions (main(), foo(), etc.)\nGlobal variables\nMetadata\nEverything else needed to compile that input\n\nIt is also known as Translation Unit or Compilation Unit, all refer to same concept - the unit of compilation."
  },
  {
    "objectID": "posts/DyT Analysis/index.html",
    "href": "posts/DyT Analysis/index.html",
    "title": "Gradient Flow and Variance Propogation Analysis of Dynamic Tanh Layer",
    "section": "",
    "text": "The “Curse of Depth” in Pre-LN transformers, identified by Sun et al. (2025), reveals that deeper layers often function as near-identity mappings, contributing minimally to the model’s performance. Their analysis found two key issues:\n\nVariance growth: In Pre-LN transformers, activation variance grows exponentially with depth, bounded by \\(\\Theta(L) \\leq \\sigma^2_{x_L} \\leq \\Theta(\\exp(L))\\)\nGradient flow limitation: The gradient norm converges to a constant \\(\\left\\|\\frac{\\partial y_L}{\\partial x_1}\\right\\|_2 \\leq M\\) rather than growing with depth, causing deeper layers to behave like identity functions.\n\nSun et al. proposed LayerNorm Scaling to address this issue. Meanwhile, Zhu et al. (2025) introduced Dynamic Tanh (DyT) as a complete replacement for normalization layers in transformers.\nThis article analyzes whether DyT can effectively mitigate the Curse of Depth by examining its effect on variance propagation and gradient flow in deep transformer networks.\n\nI’ll recommend to checkout both the original papers “The Curse of Depth in Large Language Models” and “Transformers without Normalization”."
  },
  {
    "objectID": "posts/DyT Analysis/index.html#introduction",
    "href": "posts/DyT Analysis/index.html#introduction",
    "title": "Gradient Flow and Variance Propogation Analysis of Dynamic Tanh Layer",
    "section": "",
    "text": "The “Curse of Depth” in Pre-LN transformers, identified by Sun et al. (2025), reveals that deeper layers often function as near-identity mappings, contributing minimally to the model’s performance. Their analysis found two key issues:\n\nVariance growth: In Pre-LN transformers, activation variance grows exponentially with depth, bounded by \\(\\Theta(L) \\leq \\sigma^2_{x_L} \\leq \\Theta(\\exp(L))\\)\nGradient flow limitation: The gradient norm converges to a constant \\(\\left\\|\\frac{\\partial y_L}{\\partial x_1}\\right\\|_2 \\leq M\\) rather than growing with depth, causing deeper layers to behave like identity functions.\n\nSun et al. proposed LayerNorm Scaling to address this issue. Meanwhile, Zhu et al. (2025) introduced Dynamic Tanh (DyT) as a complete replacement for normalization layers in transformers.\nThis article analyzes whether DyT can effectively mitigate the Curse of Depth by examining its effect on variance propagation and gradient flow in deep transformer networks.\n\nI’ll recommend to checkout both the original papers “The Curse of Depth in Large Language Models” and “Transformers without Normalization”."
  },
  {
    "objectID": "posts/DyT Analysis/index.html#definition-of-the-dyt-layer",
    "href": "posts/DyT Analysis/index.html#definition-of-the-dyt-layer",
    "title": "Gradient Flow and Variance Propogation Analysis of Dynamic Tanh Layer",
    "section": "1. Definition of the DyT Layer",
    "text": "1. Definition of the DyT Layer\nThe DyT layer is a parameterized nonlinearity defined as:\n\\[\n\\text{DyT}(x) = \\gamma \\odot \\tanh(\\alpha \\cdot x) + \\beta,\n\\]\nwhere:\n\n\\(x \\in \\mathbb{R}^{B \\times T \\times C}\\) is the input tensor, with \\(B\\) as batch size, \\(T\\) as sequence length, and \\(C\\) as embedding dimension,\n\\(\\alpha \\in \\mathbb{R}\\) is a learnable scalar parameter shared across all dimensions,\n\\(\\gamma \\in \\mathbb{R}^C\\) and \\(\\beta \\in \\mathbb{R}^C\\) are learnable per-dimension scaling and shift parameters,\n\\(\\odot\\) denotes element-wise multiplication,\n\\(\\tanh\\) is applied element-wise.\n\nAt initialization:\n\n\\(\\alpha = \\text{init}_\\alpha\\) (a positive scalar, e.g., \\(\\text{init}_\\alpha = 1\\)),\n\\(\\gamma = \\mathbf{1}_C\\) (a vector of ones),\n\\(\\beta = \\mathbf{0}_C\\) (a vector of zeros).\n\nThus, the initial behavior is:\n\\[\n\\text{DyT}(x) = \\tanh(\\text{init}_\\alpha \\cdot x),\n\\]\nwith \\(\\alpha\\), \\(\\gamma\\), and \\(\\beta\\) adapting during training via gradient descent."
  },
  {
    "objectID": "posts/DyT Analysis/index.html#pre-ln-transformer-architecture-with-dyt",
    "href": "posts/DyT Analysis/index.html#pre-ln-transformer-architecture-with-dyt",
    "title": "Gradient Flow and Variance Propogation Analysis of Dynamic Tanh Layer",
    "section": "2. Pre-LN Transformer Architecture with DyT",
    "text": "2. Pre-LN Transformer Architecture with DyT\nIn a standard Pre-LN Transformer, each layer () processes the input \\(x_\\ell\\) as follows:\n\\[\nz_\\ell = \\text{LN}(x_\\ell), \\quad x'_\\ell = x_\\ell + \\text{Attn}(z_\\ell), \\quad w_\\ell = \\text{LN}(x'_\\ell), \\quad x_{\\ell+1} = x'_\\ell + \\text{FFN}(w_\\ell),\n\\]\nwhere \\(\\text{Attn}\\) is the multi-head self-attention mechanism, and () is the feed-forward network.\nReplacing LN with DyT, the layer becomes:\n\\[\nz_\\ell = \\text{DyT}_1(x_\\ell), \\quad x'_\\ell = x_\\ell + \\text{Attn}(z_\\ell), \\quad w_\\ell = \\text{DyT}_2(x'_\\ell), \\quad x_{\\ell+1} = x'_\\ell + \\text{FFN}(w_\\ell),\n\\]\nwhere:\n\n\\(\\text{DyT}_1\\) has parameters \\((\\alpha_{1,\\ell}, \\gamma_{1,\\ell}, \\beta_{1,\\ell})\\),\n\\(\\text{DyT}_2\\) has parameters \\((\\alpha_{2,\\ell}, \\gamma_{2,\\ell}, \\beta_{2,\\ell})\\),\nEach DyT instance is independent per layer and per normalization step.\n\nWe assume a depth of \\(L\\) layers, with \\(x_1\\) as the initial embedding input, and analyze the dynamics from \\(x_1\\) to \\(x_{L+1}\\)."
  },
  {
    "objectID": "posts/DyT Analysis/index.html#variance-propagation-analysis",
    "href": "posts/DyT Analysis/index.html#variance-propagation-analysis",
    "title": "Gradient Flow and Variance Propogation Analysis of Dynamic Tanh Layer",
    "section": "3. Variance Propagation Analysis",
    "text": "3. Variance Propagation Analysis\nTo assess the Curse of Depth, we first examine how the variance of activations \\(\\sigma_\\ell^2 = \\text{Var}(x_\\ell)\\) evolves across layers. We assume:\n\nElement-wise independence within \\(x_\\ell\\) (a standard simplification in initialization studies),\nZero-mean inputs per dimension at initialization (e.g., \\(x_1 \\sim \\mathcal{N}(0, \\sigma_1^2 I_C)\\)),\nAttention and FFN sub-layers are initialized to approximately preserve variance (e.g., via Xavier initialization).\n\n\n3.1. Variance of DyT Output\nConsider a single dimension \\(x \\sim \\mathcal{N}(0, \\sigma^2)\\). The DyT output is:\n\\[\ny = \\gamma \\cdot \\tanh(\\alpha x) + \\beta.\n\\]\nSince \\(\\beta\\) is a constant shift, it does not affect variance:\n\\[\n\\text{Var}(y) = \\gamma^2 \\cdot \\text{Var}(\\tanh(\\alpha x)).\n\\]\nWe need to compute \\(\\text{Var}(\\tanh(\\alpha x))\\):\n\\[\n\\text{Var}(\\tanh(\\alpha x)) = \\mathbb{E}[\\tanh(\\alpha x)^2] - \\mathbb{E}[\\tanh(\\alpha x)]^2.\n\\]\nSince \\(\\tanh\\) is an odd function and \\(x\\) is symmetric around zero:\n\\[\n\\mathbb{E}[\\tanh(\\alpha x)] = 0,\n\\]\n\n\n\n\n\n\nNoteProof of \\(\\mathbb{E}[\\tanh(\\alpha x)] = 0\\)\n\n\n\n\n\nFor \\(x \\sim \\mathcal{N}(0, \\sigma^2)\\), the expectation of \\(\\tanh(\\alpha x)\\) is:\n\\[\n\\mathbb{E}[\\tanh(\\alpha x)] = \\int_{-\\infty}^{\\infty} \\tanh(\\alpha x) \\cdot \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{x^2}{2\\sigma^2}} dx\n\\]\nSince \\(\\tanh(-z) = -\\tanh(z)\\) (odd function) and the normal distribution is symmetric around 0:\n\\[\n\\int_{-\\infty}^{0} \\tanh(\\alpha x) \\cdot \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{x^2}{2\\sigma^2}} dx + \\int_{0}^{\\infty} \\tanh(\\alpha x) \\cdot \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{x^2}{2\\sigma^2}} dx = 0\n\\]\nThe first integral equals the negative of the second integral, so they sum to zero.\n\n\n\nthus:\n\\[\n\\text{Var}(\\tanh(\\alpha x)) = \\mathbb{E}[\\tanh(\\alpha x)^2].\n\\]\nFor \\(x \\sim \\mathcal{N}(0, \\sigma^2)\\), let \\(u = \\frac{x}{\\sigma} \\sim \\mathcal{N}(0, 1)\\), so \\(\\alpha x = \\alpha \\sigma u\\), and:\n\\[\n\\mathbb{E}[\\tanh(\\alpha x)^2] = \\mathbb{E}[\\tanh(\\alpha \\sigma u)^2] = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty \\tanh(\\alpha \\sigma u)^2 e^{-u^2/2} du.\n\\]\nThis integral lacks a closed form but can be bounded: - Small \\(\\alpha \\sigma\\): Using the approximation \\(\\tanh(z) \\approx z\\) for small \\(z\\):\n\\[\n\\tanh(\\alpha x) \\approx \\alpha x, \\quad \\mathbb{E}[\\tanh(\\alpha x)^2] \\approx \\mathbb{E}[(\\alpha x)^2] = \\alpha^2 \\sigma^2.\n\\]\n\nLarge \\(\\alpha \\sigma\\): As \\(|\\alpha x| \\to \\infty\\), \\(\\tanh(\\alpha x) \\to \\text{sign}(x)\\), so \\(\\tanh(\\alpha x)^2 \\to 1\\), and:\n\n\\[\n\\mathbb{E}[\\tanh(\\alpha x)^2] \\to 1.\n\\]\nMore precisely, using the identity \\(\\tanh(z)^2 = 1 - \\text{sech}(z)^2\\), and symmetry:\n\\[\n\\mathbb{E}[\\tanh(\\alpha x)^2] = 1 - 2 \\cdot \\frac{1}{\\sqrt{2\\pi}} \\int_0^\\infty \\text{sech}(\\alpha \\sigma u)^2 e^{-u^2/2} du.\n\\]\nSince \\(0 \\leq \\text{sech}(z)^2 \\leq 1\\), we have:\n\\[\n0 \\leq \\mathbb{E}[\\tanh(\\alpha x)^2] \\leq 1,\n\\]\nwith the value increasing monotonically from \\(\\alpha^2 \\sigma^2\\) (when \\(\\alpha \\sigma \\ll 1\\)) to 1 (when \\(\\alpha \\sigma \\gg 1\\)). At initialization, \\(\\gamma = 1\\), so:\n\\[\n\\text{Var}(\\text{DyT}(x)) \\leq 1.\n\\]\n\n\n3.2. Variance Across a Transformer Layer\nNow, propagate variance through one layer:\n\nInput: \\(\\text{Var}(x_\\ell) = \\sigma_\\ell^2\\).\n\\(DyT_1\\): \\(z_\\ell = \\text{DyT}_1(x_\\ell)\\), so:\n\n\\[\n\\text{Var}(z_\\ell) = \\mathbb{E}[\\tanh(\\alpha_{1,\\ell} x_\\ell)^2] \\leq 1.\n\\]\n\nAttention Step: \\(x'_\\ell = x_\\ell + \\text{Attn}(z_\\ell)\\).\n\nAssume \\(\\text{Attn}\\) preserves variance: \\(\\text{Var}(\\text{Attn}(z_\\ell)) = \\text{Var}(z_\\ell) \\leq 1\\).\nAssuming independence between \\(x_\\ell\\) and \\(\\text{Attn}(z_\\ell)\\) (an approximation):\n\n\n\\[\n\\text{Var}(x'_\\ell) = \\sigma_\\ell^2 + \\text{Var}(\\text{Attn}(z_\\ell)) \\leq \\sigma_\\ell^2 + 1.\n\\]\n\n\\(DyT_2\\): \\(w_\\ell = \\text{DyT}_2(x'_\\ell)\\), so:\n\n\\[\n\\text{Var}(w_\\ell) = \\mathbb{E}[\\tanh(\\alpha_{2,\\ell} x'_\\ell)^2] \\leq 1.\n\\]\n\nFFN Step: \\(x_{\\ell+1} = x'_\\ell + \\text{FFN}(w_\\ell)\\).\n\nAssume \\(\\text{FFN}\\) preserves variance: \\(\\text{Var}(\\text{FFN}(w_\\ell)) = \\text{Var}(w_\\ell) \\leq 1\\).\nThus:\n\n\n\\[\n\\text{Var}(x_{\\ell+1}) = \\text{Var}(x'_\\ell) + \\text{Var}(\\text{FFN}(w_\\ell)) \\leq (\\sigma_\\ell^2 + 1) + 1 = \\sigma_\\ell^2 + 2.\n\\]\n\n\n3.3. Recurrence and Total Variance Growth\nThe recurrence relation is:\n\\[\n\\sigma_{\\ell+1}^2 \\leq \\sigma_\\ell^2 + 2.\n\\]\nSolving with initial condition \\(\\sigma_1^2\\):\n\\[\n\\sigma_2^2 \\leq \\sigma_1^2 + 2,\n\\]\n\\[\n\\sigma_3^2 \\leq \\sigma_2^2 + 2 \\leq \\sigma_1^2 + 4,\n\\]\n\\[\n\\sigma_{\\ell+1}^2 \\leq \\sigma_1^2 + 2\\ell.\n\\]\nFor the final layer \\(x_{L+1}\\):\n\\[\n\\sigma_{L+1}^2 \\leq \\sigma_1^2 + 2L.\n\\]\nThus, the variance grows at most linearly with depth:\n\\[\n\\sigma_{L+1}^2 = O(L).\n\\]\nThis is a significant improvement over Pre-LN Transformers without normalization scaling, where variance can grow exponentially (\\(\\Theta(\\exp(L))\\)) in pathological cases, though empirical growth is often sub-exponential."
  },
  {
    "objectID": "posts/DyT Analysis/index.html#gradient-flow-analysis",
    "href": "posts/DyT Analysis/index.html#gradient-flow-analysis",
    "title": "Gradient Flow and Variance Propogation Analysis of Dynamic Tanh Layer",
    "section": "4. Gradient Flow Analysis",
    "text": "4. Gradient Flow Analysis\nNext, we analyze gradient propagation to determine if DyT prevents deeper layers from becoming identity-like. We compute the Jacobian \\(\\frac{\\partial x_{L+1}}{\\partial x_1}\\) and its norm.\n\n4.1. Derivative of DyT\nFor \\(y = \\text{DyT}(x)\\), per dimension:\n\\[\n\\frac{\\partial y_c}{\\partial x_c} = \\gamma_c \\cdot \\alpha \\cdot \\frac{\\partial \\tanh(\\alpha x_c)}{\\partial (\\alpha x_c)} = \\gamma_c \\cdot \\alpha \\cdot (1 - \\tanh(\\alpha x_c)^2).\n\\]\nUsing \\(1 - \\tanh(z)^2 = \\text{sech}(z)^2\\):\n\\[\n\\left| \\frac{\\partial y_c}{\\partial x_c} \\right| = |\\gamma_c| \\cdot |\\alpha| \\cdot \\text{sech}(\\alpha x_c)^2,\n\\]\nwhere \\(0 &lt; \\text{sech}(z)^2 \\leq 1\\). The Jacobian is diagonal:\n\\[\n\\frac{\\partial y}{\\partial x} = \\text{diag}(\\gamma_c \\cdot \\alpha \\cdot (1 - \\tanh(\\alpha x_c)^2)),\n\\]\n\\[\n\\left\\| \\frac{\\partial y}{\\partial x} \\right\\|_2 = \\max_c |\\gamma_c \\cdot \\alpha \\cdot (1 - \\tanh(\\alpha x_c)^2)| \\leq |\\alpha| \\cdot \\max_c |\\gamma_c|.\n\\]\n\nSmall \\(|x_c|\\): \\(\\tanh(\\alpha x_c) \\approx 0\\), so \\(\\frac{\\partial y_c}{\\partial x_c} \\approx \\gamma_c \\cdot \\alpha\\).\nLarge \\(|x_c|\\): \\(\\tanh(\\alpha x_c) \\to \\pm 1\\), so \\(\\frac{\\partial y_c}{\\partial x_c} \\to 0\\).\n\n\n\n\n\n\n\nNoteWhy is the Jacobian diagonal?\n\n\n\n\n\nThe Jacobian matrix of DyT is diagonal because the tanh operation is applied element-wise. Mathematically, for an input vector \\(x \\in \\mathbb{R}^d\\) and output vector \\(y = \\text{DyT}(x) \\in \\mathbb{R}^d\\):\n\\[\\text{DyT}(x)_i = \\gamma_i \\tanh(\\alpha x_i) + \\beta_i\\]\nThe partial derivative of the \\(i\\)-th output with respect to the \\(j\\)-th input is:\n\\[\\frac{\\partial y_i}{\\partial x_j} = \\begin{cases}\n\\gamma_i \\cdot \\alpha \\cdot (1 - \\tanh^2(\\alpha x_i)) & \\text{if } i = j \\\\\n0 & \\text{if } i \\neq j\n\\end{cases}\\]\nThis creates a diagonal Jacobian matrix where only the entries along the main diagonal are non-zero:\n\\[J = \\begin{bmatrix}\n\\frac{\\partial y_1}{\\partial x_1} & 0 & \\cdots & 0 \\\\\n0 & \\frac{\\partial y_2}{\\partial x_2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\frac{\\partial y_d}{\\partial x_d}\n\\end{bmatrix}\\]\nThe diagonal nature of this Jacobian significantly simplifies our analysis of gradient flow through the network.\n\n\n\n\n\n4.2. Jacobian of a Transformer Layer\nFor \\(x_{\\ell+1} = x'_\\ell + \\text{FFN}(\\text{DyT}_2(x'_\\ell))\\), where \\(x'_\\ell = x_\\ell + \\text{Attn}(\\text{DyT}_1(x_\\ell))\\):\n\\[\n\\frac{\\partial x_{\\ell+1}}{\\partial x_\\ell} = \\frac{\\partial x_{\\ell+1}}{\\partial x'_\\ell} \\cdot \\frac{\\partial x'_\\ell}{\\partial x_\\ell}.\n\\]\n\nAttention Step:\n\n\\[\n\\frac{\\partial x'_\\ell}{\\partial x_\\ell} = I + \\frac{\\partial \\text{Attn}}{\\partial z_\\ell} \\cdot \\frac{\\partial z_\\ell}{\\partial x_\\ell},\n\\]\nwhere \\(\\frac{\\partial z_\\ell}{\\partial x_\\ell} = \\text{diag}(\\gamma_{1,\\ell,c} \\cdot \\alpha_{1,\\ell} \\cdot (1 - \\tanh(\\alpha_{1,\\ell} x_{\\ell,c})^2))\\), and:\n\\[\n\\left\\| \\frac{\\partial z_\\ell}{\\partial x_\\ell} \\right\\|_2 \\leq |\\alpha_{1,\\ell}| \\cdot \\max_c |\\gamma_{1,\\ell,c}| \\cdot \\max_z (1 - \\tanh(z)^2) = |\\alpha_{1,\\ell}| \\cdot \\max_c |\\gamma_{1,\\ell,c}|.\n\\]\nAssume \\(\\left\\| \\frac{\\partial \\text{Attn}}{\\partial z_\\ell} \\right\\|_2 \\leq A\\) (via initialization):\n\\[\n\\left\\| \\frac{\\partial x'_\\ell}{\\partial x_\\ell} \\right\\|_2 \\leq 1 + A \\cdot |\\alpha_{1,\\ell}| \\cdot \\max_c |\\gamma_{1,\\ell,c}|.\n\\]\n\nFFN Step:\n\n\\[\n\\frac{\\partial x_{\\ell+1}}{\\partial x'_\\ell} = I + \\frac{\\partial \\text{FFN}}{\\partial w_\\ell} \\cdot \\frac{\\partial w_\\ell}{\\partial x'_\\ell},\n\\]\n\\[\n\\left\\| \\frac{\\partial w_\\ell}{\\partial x'_\\ell} \\right\\|_2 \\leq |\\alpha_{2,\\ell}| \\cdot \\max_c |\\gamma_{2,\\ell,c}|,\n\\]\n\\[\n\\left\\| \\frac{\\partial x_{\\ell+1}}{\\partial x'_\\ell} \\right\\|_2 \\leq 1 + B \\cdot |\\alpha_{2,\\ell}| \\cdot \\max_c |\\gamma_{2,\\ell,c}|,\n\\]\nwhere \\(\\left\\| \\frac{\\partial \\text{FFN}}{\\partial w_\\ell} \\right\\|_2 \\leq B\\).\n\nTotal Layer Jacobian:\n\n\\[\n\\left\\| \\frac{\\partial x_{\\ell+1}}{\\partial x_\\ell} \\right\\|_2 \\leq \\left(1 + A \\cdot |\\alpha_{1,\\ell}| \\cdot \\max_c |\\gamma_{1,\\ell,c}|\\right) \\left(1 + B \\cdot |\\alpha_{2,\\ell}| \\cdot \\max_c |\\gamma_{2,\\ell,c}|\\right).\n\\]\n\n\n4.3. Effect of Saturation\nAs \\(\\sigma_\\ell^2\\) grows linearly (\\(\\sigma_\\ell^2 \\leq \\sigma_1^2 + 2(\\ell-1)\\)), \\(|x_{\\ell,c}|\\) becomes large with high probability. For large \\(z\\):\n\\[\n1 - \\tanh(z)^2 \\approx 4 e^{-2|z|},\n\\]\nso \\(\\frac{\\partial z_\\ell}{\\partial x_\\ell} \\to 0\\), and:\n\\[\n\\left\\| \\frac{\\partial x'_\\ell}{\\partial x_\\ell} \\right\\|_2 \\to 1, \\quad \\left\\| \\frac{\\partial x_{\\ell+1}}{\\partial x'_\\ell} \\right\\|_2 \\to 1,\n\\]\n\\[\n\\left\\| \\frac{\\partial x_{\\ell+1}}{\\partial x_\\ell} \\right\\|_2 \\to 1.\n\\]\n\n\n4.4. Total Gradient Norm\n\\[\n\\left\\| \\frac{\\partial x_{L+1}}{\\partial x_1} \\right\\|_2 \\leq \\prod_{\\ell=1}^L \\left\\| \\frac{\\partial x_{\\ell+1}}{\\partial x_\\ell} \\right\\|_2.\n\\]\nIn shallow layers (\\(\\sigma_\\ell^2\\) small), the norm may exceed 1, but in deeper layers (\\(\\sigma_\\ell^2\\) large), it approaches 1 due to saturation. Assuming a transition depth \\(\\ell_0\\) where saturation dominates:\n\\[\n\\left\\| \\frac{\\partial x_{L+1}}{\\partial x_1} \\right\\|_2 \\approx \\left( \\prod_{\\ell=1}^{\\ell_0} \\left(1 + O(\\alpha_\\ell)\\right) \\right) \\cdot 1^{L - \\ell_0} = O(1),\n\\]\nindicating a bounded gradient norm."
  },
  {
    "objectID": "posts/DyT Analysis/index.html#comparison-to-layernorm-and-layernorm-scaling",
    "href": "posts/DyT Analysis/index.html#comparison-to-layernorm-and-layernorm-scaling",
    "title": "Gradient Flow and Variance Propogation Analysis of Dynamic Tanh Layer",
    "section": "5. Comparison to LayerNorm and LayerNorm Scaling",
    "text": "5. Comparison to LayerNorm and LayerNorm Scaling\n\nPre-LN with LN: Variance grows sub-exponentially, and \\(\\left\\| \\frac{\\partial x_L}{\\partial x_1} \\right\\|_2 \\to O(1)\\), causing identity-like behavior.\nLayerNorm Scaling: Scales LN by \\(\\frac{1}{\\sqrt{\\ell}}\\), reducing variance growth and allowing \\(\\left\\| \\frac{\\partial x_L}{\\partial x_1} \\right\\|_2 = \\Theta(L)\\), enhancing expressivity.\nDyT: Variance grows linearly (\\(O(L)\\)), but gradient norms stabilize at \\(O(1)\\) due to tanh saturation."
  },
  {
    "objectID": "posts/DyT Analysis/index.html#conclusion",
    "href": "posts/DyT Analysis/index.html#conclusion",
    "title": "Gradient Flow and Variance Propogation Analysis of Dynamic Tanh Layer",
    "section": "6. Conclusion",
    "text": "6. Conclusion\nDyT bounds variance growth to \\(O(L)\\), better than Pre-LN’s potential exponential upper bound, due to tanh’s saturation. However, this same saturation causes \\(\\frac{\\partial D_y T}{\\partial x} \\to 0\\) for large inputs, making layer derivatives approach identity in deep layers, akin to Pre-LN’s Curse of Depth. Unlike LayerNorm Scaling, DyT lacks a depth-dependent mechanism to sustain gradient growth, so it doesn’t fully resolve the issue, though it mitigates variance explosion to some extent."
  },
  {
    "objectID": "posts/Self-Attention Mimicking Gradient Descent/index.html",
    "href": "posts/Self-Attention Mimicking Gradient Descent/index.html",
    "title": "Self-Attention Mimicking Gradient Descent",
    "section": "",
    "text": "Self-Attention Mimicking Gradient Descent\n\n\n\nThis section of paper Uncovering mesa-optimization algorithms in Transformers presents a theoretical construction where a linear self-attention layer in a Transformer architecture can mimic a single step of gradient descent for a linear regression task.\n\nToken Construction (from the paper):\n\n\nTokens: A set of tokens \\(E_T\\) is constructed with \\(T = N\\) such that \\(e_t = (y_{\\tau,i}, x_{\\tau,i})\\), where \\(y_{\\tau,i}\\) and \\(x_{\\tau,i}\\) are concatenated.\nQuery Token: A query token \\(e_{T+1}\\) is created as \\(e_{T+1} = (-W_0 x_{\\tau,\\text{test}}, x_{\\tau,\\text{test}})\\) . This token represents the test input for which a prediction is to be made.\n\n\nWhy doing \\(-W_0 x_{\\tau,\\text{test}}\\) ? \\(W_o\\) represent the model’s initial weights, multiplying it with \\(x_{\\tau,\\text{test}}\\) provides initial context for prediction (basically giving a perspective to start with). The \\(-_(ve)\\) sign is to align with the GD update, where we move in the direction opposite to the gradient. So we can say that The initial negative prediction in the query token provides a starting point, and the self-attention mechanism’s update to this prediction results in a new prediction that mimics\n\n\n\nConditions (from the paper):\n\n\nAll bias terms are zero\n\nbasically only using the model weights without any bias term\n\n\\[\nW^T_k W_q = \\begin{bmatrix} 0 & 0 \\\\ 0 & I_x \\end{bmatrix}\n\\]\n\nAs per my understanding, the relevance of this condition depends is as following\n\nSo, I’ll write the equations for calculating attention weights and they are self-explanatory to be honest (also this is my fav interpretation of this condition)\n\\[\ne = \\begin{bmatrix} y_1 \\\\ x_1 \\\\ x_2 \\end{bmatrix} \\\\\n\\text{ and }q = \\begin{bmatrix} q_y \\\\ q_{x1} \\\\ q_{x2} \\end{bmatrix}\n\\]\n\\[k = W_k e\\]\n\\[q' = W_q q\\]\n\\[\\text{attention weight} \\propto k^T q'\\]\nGiven the condition, this becomes: \\[\\text{weight} \\propto e^T W^T_k W_q q\\]\nNow, plugging in the condition\n\\[W^T_k W_q = \\begin{bmatrix} 0 & 0 \\\\ 0 & I_x \\end{bmatrix}\\]\nthe interaction simplifies to: \\[\n\\text{weight} \\propto \\begin{bmatrix} y_1 & x_1 & x_2 \\end{bmatrix} \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 1 & 0\\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} q_y \\\\ q_{x1} \\\\ q_{x2} \\end{bmatrix}\n\\] \\[\n\\text{weight} \\propto \\begin{bmatrix} 0 & x_1 & x_2 \\end{bmatrix} \\begin{bmatrix} q_y \\\\ q_{x1} \\\\ q_{x2} \\end{bmatrix}\n\\] \\[\\text{weight} \\propto x_1 q_{x1} + x_2 q_{x2}\\]\nSo what we can see here is that, the attention-weights are proportional to the dot-product of x-component of inputs and query (rejecting the influence of y-component)\n\n\\[ P W_v = \\begin{bmatrix} -\\eta I_y & \\eta W_0 \\\\ 0 & 0 \\end{bmatrix}\\]\n\nThis is the another condition and lets try to understand what it means, here \\(\\eta\\) is the learning-rate, \\(*P*\\) is a projection matrix, and \\(*W_v*\\) is the weight matrix for the “values”.\nBefore diving into the interpretation of this condition, we should know that the Linear Self Attention can be represented as \\(P V_{t} K^T_{t} q_{t}\\) (considering 1 head for simplicity) here \\(P\\) is just the Projection matrix and the rest of the term is exact same as calculating attention.\nThe matrix product \\(PW_v\\) determines how the values contribute to the updated query token. Now lets try to understand it\nIn this interpretation, I’ll again try to resolve this by solving equations, since our new value_matrix is \\(PW_v\\) i.e we can calculate values of input token as\n\\[v = PW_ve\\]\nwhere \\(e = \\begin{bmatrix} y_1 \\\\ x_1 \\\\ x_2 \\end{bmatrix}\\) and \\(P W_v = \\begin{bmatrix} -\\eta & \\eta W_{01} & \\eta W_{02} \\\\ 0 & 0 & 0\\\\ 0 & 0 & 0\\end{bmatrix}\\)\n\\[v = -\\eta y + \\eta W_{01}x_1 +\\eta W_{02}x_2\\]\nBased on this equation we can interpret the condition as:\n\nThe Upper Left Block \\((-\\eta I_y)\\): This block scales the y-component of the values (the outputs) by −η. In the context of gradient descent, the update is proportional to the negative gradient. This block captures the idea that the update to our model’s prediction should be in the opposite direction of the error (difference between prediction and actual output). Multiplying by −η ensures that if our model’s prediction is too high, it gets adjusted downwards, and if it’s too low, it gets adjusted upwards.\nThe Upper Right Block (\\(\\eta W_0\\)): This block scales the x-component of the values (the inputs) and then multiplies by the initial weight \\(*W_0\\).* This captures the contribution of the inputs to the gradient of the loss with respect to the model parameters. In other words, it represents how much each input feature contributes to the error. Multiplying by \\(*W_0*\\) gives the model’s initial reliance on each feature, and the entire product indicates how the model should adjust its reliance on each feature to minimize the error.\nThe Lower Blocks (0 matrices): These blocks ensure that the x-component of the updated query token remains unchanged. This is consistent with the idea that the input part of our test example doesn’t change; only our model’s prediction (or representation) of it does.\n\n\nThat covers all the required conditions, now let’s see that satisfying these conditions, how the weights of the self-attention will contain the gradient of the loss of a Linear Regression objective.\n\n\nGradient Descent in Linear Regression:\n\nFor a linear regression task, the gradient descent update rule is: \\[\\Delta W_0 = \\eta \\sum_{i=1}^{N} (y_{\\tau,i} - W_0 x_{\\tau,i}) x^T_{\\tau,i}\\]\n\n\nLinear Self-Attention Layer:\n\nGiven the conditions, the self-attention mechanism computes the weighted sum of values based on the similarity (dot product) of the query with the keys.\nTo prove: The weights in the attention mechanism will effectively compute the gradient of the loss with respect to the model parameters.\nLet’s move to the final derivation:\n\nAttention Weights Calculation: Given the second condition, the attention weights are determined by the dot product of the query with the keys: \\[\\alpha_t = q \\cdot k_t\\] Where \\(q\\) is the query, and \\(k_t\\) is the key for the t-th token.\nValue Update: The update to the query token using the attention mechanism is: \\[e_{T+1,\\text{new}} = \\sum_{t=1}^{T} \\alpha_t v_t\\] Where \\(v_t\\) is the value for the t-th token.\n\nGiven the third condition, the value for each token is: \\[v_t = P W_v e_t\\] Substituting this in, we get: \\[e_{T+1,\\text{new}} = \\sum_{t=1}^{T} \\alpha_t P W_v e_t\\]\nMatching with Gradient Descent: Given the attention weights \\((\\alpha_t)\\) and the gradient descent update rule, the y-component of the updated query token is: \\[\ne_{T+1,\\text{new,y}} = \\sum_{t=1}^{T} \\alpha_t (-\\eta y_{\\tau,i} + \\eta W_0 x_{\\tau,i}) \\\\\ne_{T+1,\\text{new,y}} = \\sum_{t=1}^{T} \\alpha_t \\eta (W_0 x_{\\tau,i} - y_{\\tau,i})\n\\]\nNow, comparing this with the gradient descent update rule, we can see that the term inside the summation \\((W_0x_τ,_iy_τ,_i)\\) is essentially the error in prediction for the training data.\nGiven that the attention weights \\(α_t\\) effectively compute a weighted version of this error, the entire equation can be seen as a weighted sum of errors, which is analogous to the gradient in gradient descent.\n\n\nApproximation:\n\nNow, for the test input \\(x_{\\tau,\\text{test}}\\), the prediction using the initial weights \\(W_0\\) is \\(W_0 x_{\\tau,\\text{test}}\\). After one step of gradient descent, the prediction becomes: \\[(W_0 - \\Delta W_0) x_{\\tau,\\text{test}}\\]\nGiven that the self-attention mechanism’s update to the query token is designed to mimic one step of gradient descent, we can approximate: \\[e_{T+1,\\text{new,y}} \\approx (W_0 - \\Delta W_0) x_{\\tau,\\text{test}}\\]\nThis approximation captures the essence of the theoretical construction: the self-attention mechanism updates the query token in a way that mimics the behavior of gradient descent\nAlso the rearrangement and approximation are based on the insight that the self-attention mechanism’s update to the query token aligns with the gradient descent update rule for linear regression, allowing the mechanism to make predictions consistent with gradient descent optimization."
  },
  {
    "objectID": "posts/Understanding Perplexity/index.html",
    "href": "posts/Understanding Perplexity/index.html",
    "title": "Understanding Perplexity",
    "section": "",
    "text": "Recently, I was reading the Chapter 5 (Pretraining) of the book “Build a Large Language Model (From Scratch)” by Sebastian Raschka. I stumbled upon an intriguing interpretation of perplexity. The author noted:\n\n“Perplexity is often considered more interpretable than the raw loss value because it signifies the effective vocabulary size about which the model is uncertain at each step.”\n\nIn simple words, If for some model the perplexity comes out to be \\(N\\) then it means that the model is \\(N\\) tokens uncertain about the correct next-token, it is considering all the \\(N\\) tokens as the potential candidate for the output token.\nThis statement resonated with me, as I had always viewed perplexity as just a performance metric. I began to wonder: can we mathematically derive this interpretation? Does the underlying math support this idea?\nLet’s delve into the equations and explore how perplexity relates to the model’s uncertainty about the next token in a sequence.\n\n\nIn language modeling, cross-entropy loss is a critical metric that helps us evaluate how well a model predicts the next token in a sequence. For a sequence of tokens \\(x = (x_1, x_2, ..., x_T)\\), the cross-entropy loss is calculated as:\n\\[\n\\mathcal{L} = - \\frac{1}{T} \\sum_{t=1}^{T} \\log P(x_t | \\mathbf{x}_{&lt;t})\n\\]\nwhere:\n\n\\(T\\) is the total number of tokens in the sequence.\n\n\n\\(P(x_t | \\mathbf{x}_{&lt;t})\\) is the predicted probability of the actual token \\(x_t\\) given the preceding context \\(\\mathbf{x}_{&lt;t}\\).\n\n This formulation averages the negative log-likelihood across all tokens, providing a measure of how well the model’s predictions align with the true tokens.\n\n\n\nPerplexity serves as a complementary metric to cross-entropy loss and is defined as the exponentiation of the loss:\n\\[\n\\text{Perplexity} = \\exp(\\mathcal{L})\n\\]\nThis formulation provides a more interpretable value, as it represents the effective number of choices the model considers when predicting the next token. A lower perplexity indicates higher confidence in predictions, while a higher perplexity signifies greater uncertainty.\n\n\n\n\n\n\nNote\n\n\n\nBefore going into maths, lets understand one thing\nIntuitively, for a completely uncertain model, selection for some next-token can be any from the whole vocabulary with each token having same probability of being the next token\n\n\n\n\n\nTo understand the interpretation of perplexity in terms of effective vocabulary size, let’s consider an extreme case where the model is completely uncertain about the next token. In this scenario, the model assigns equal probability to every token in the vocabulary of size \\(V\\). Thus, the probability of each token can be expressed as:\n\\[\nP(x_t | \\mathbf{x}_{&lt;t}) = \\frac{1}{V}\n\\]\nNow, substituting this uniform probability into the cross-entropy loss equation, we get:\n\\[\n\\mathcal{L} = - \\frac{1}{T} \\sum_{t=1}^{T} \\log P(x_t | \\mathbf{x}_{&lt;t}) = - \\log \\frac{1}{V} = \\log V\n\\]\nHere, \\(-\\log P(x_t | \\mathbf{x}_{&lt;t})\\) reflects the loss incurred for each token when the model is entirely uncertain.\n\n\n\nNext, we can use the perplexity formula to analyze this situation:\n\\[\n\\text{Perplexity} = \\exp(\\mathcal{L}) = \\exp(\\log V) = V\n\\]\nThis result reveals a fascinating insight: when the model is completely uncertain, the perplexity is exactly equal to the size of the vocabulary \\(V\\).\n\n\nNow, what does this mean in terms of interpretation? When the perplexity equals \\(V\\), it indicates that the model is effectively considering all \\(V\\) tokens as potential candidates for the next token, reflecting a state of maximum uncertainty.\nOn the other hand, if the model has a lower perplexity, say 100, it means that the model behaves as if it is uncertain only among 100 tokens. This aligns perfectly with the statement from Raschka’s book: perplexity signifies the effective vocabulary size about which the model is uncertain at each step."
  },
  {
    "objectID": "posts/Understanding Perplexity/index.html#cross-entropy-loss-a-quick-recap",
    "href": "posts/Understanding Perplexity/index.html#cross-entropy-loss-a-quick-recap",
    "title": "Understanding Perplexity",
    "section": "",
    "text": "In language modeling, cross-entropy loss is a critical metric that helps us evaluate how well a model predicts the next token in a sequence. For a sequence of tokens \\(x = (x_1, x_2, ..., x_T)\\), the cross-entropy loss is calculated as:\n\\[\n\\mathcal{L} = - \\frac{1}{T} \\sum_{t=1}^{T} \\log P(x_t | \\mathbf{x}_{&lt;t})\n\\]\nwhere:\n\n\\(T\\) is the total number of tokens in the sequence.\n\n\n\\(P(x_t | \\mathbf{x}_{&lt;t})\\) is the predicted probability of the actual token \\(x_t\\) given the preceding context \\(\\mathbf{x}_{&lt;t}\\).\n\n This formulation averages the negative log-likelihood across all tokens, providing a measure of how well the model’s predictions align with the true tokens."
  },
  {
    "objectID": "posts/Understanding Perplexity/index.html#defining-perplexity",
    "href": "posts/Understanding Perplexity/index.html#defining-perplexity",
    "title": "Understanding Perplexity",
    "section": "",
    "text": "Perplexity serves as a complementary metric to cross-entropy loss and is defined as the exponentiation of the loss:\n\\[\n\\text{Perplexity} = \\exp(\\mathcal{L})\n\\]\nThis formulation provides a more interpretable value, as it represents the effective number of choices the model considers when predicting the next token. A lower perplexity indicates higher confidence in predictions, while a higher perplexity signifies greater uncertainty.\n\n\n\n\n\n\nNote\n\n\n\nBefore going into maths, lets understand one thing\nIntuitively, for a completely uncertain model, selection for some next-token can be any from the whole vocabulary with each token having same probability of being the next token"
  },
  {
    "objectID": "posts/Understanding Perplexity/index.html#analyzing-the-uniform-distribution-case",
    "href": "posts/Understanding Perplexity/index.html#analyzing-the-uniform-distribution-case",
    "title": "Understanding Perplexity",
    "section": "",
    "text": "To understand the interpretation of perplexity in terms of effective vocabulary size, let’s consider an extreme case where the model is completely uncertain about the next token. In this scenario, the model assigns equal probability to every token in the vocabulary of size \\(V\\). Thus, the probability of each token can be expressed as:\n\\[\nP(x_t | \\mathbf{x}_{&lt;t}) = \\frac{1}{V}\n\\]\nNow, substituting this uniform probability into the cross-entropy loss equation, we get:\n\\[\n\\mathcal{L} = - \\frac{1}{T} \\sum_{t=1}^{T} \\log P(x_t | \\mathbf{x}_{&lt;t}) = - \\log \\frac{1}{V} = \\log V\n\\]\nHere, \\(-\\log P(x_t | \\mathbf{x}_{&lt;t})\\) reflects the loss incurred for each token when the model is entirely uncertain."
  },
  {
    "objectID": "posts/Understanding Perplexity/index.html#connecting-loss-and-perplexity",
    "href": "posts/Understanding Perplexity/index.html#connecting-loss-and-perplexity",
    "title": "Understanding Perplexity",
    "section": "",
    "text": "Next, we can use the perplexity formula to analyze this situation:\n\\[\n\\text{Perplexity} = \\exp(\\mathcal{L}) = \\exp(\\log V) = V\n\\]\nThis result reveals a fascinating insight: when the model is completely uncertain, the perplexity is exactly equal to the size of the vocabulary \\(V\\).\n\n\nNow, what does this mean in terms of interpretation? When the perplexity equals \\(V\\), it indicates that the model is effectively considering all \\(V\\) tokens as potential candidates for the next token, reflecting a state of maximum uncertainty.\nOn the other hand, if the model has a lower perplexity, say 100, it means that the model behaves as if it is uncertain only among 100 tokens. This aligns perfectly with the statement from Raschka’s book: perplexity signifies the effective vocabulary size about which the model is uncertain at each step."
  },
  {
    "objectID": "posts/Cooperative Groups/index.html#motivation",
    "href": "posts/Cooperative Groups/index.html#motivation",
    "title": "CUDA Notes: Cooperative Groups",
    "section": "Motivation",
    "text": "Motivation\nTill now we studied\n\n__syncthreads() block level synchronization barrier\nShared Memory, also block level\nWarp Shuffle Premitives, they are warp level\n\nSo this allow a limited amount of functionalities for threads to inter communicate or cooperate and restrict to work at block or warp level. Cooperative Groups provide more flexibility and go beyond thread block level.\nCooperative Groups is an extension to the CUDA programming model, introduced in CUDA 9, for organizing groups of communicating threads. Cooperative Groups allows developers to express the granularity at which threads are communicating, helping them to express richer, more efficient parallel decompositions.\nBefore them\n\nYou couldn’t easily synchronize smaller, logical groups of threads within a block. For instance, if you divided a block’s work into independent sections (like tiles), you couldn’t synchronize just the threads working on one tile without halting all threads in the block using __syncthreads(). While developers could implement custom synchronization using atomic operations and shared memory flags, this was complex, error-prone, and often less efficient.\nThere was no built-in, safe way to synchronize all threads across all blocks within a single kernel launch. If you needed a point where all work across the entire GPU grid had to be finished before the next phase could begin (e.g., after a global reduction step before a subsequent calculation), the standard approach was:\n\nFinish the first kernel.\nReturn control to the CPU.\nLaunch a second kernel for the next phase. The boundary between kernel launches acted as an implicit grid-wide synchronization point. However, launching kernels has overhead (typically microseconds), which can become significant if you need many synchronization points. Custom attempts at in-kernel grid sync were difficult and often relied on assumptions about hardware scheduling that could lead to deadlocks (where threads wait indefinitely for others that might never run concurrently)."
  },
  {
    "objectID": "posts/Cooperative Groups/index.html#what-cooperative-groups-are",
    "href": "posts/Cooperative Groups/index.html#what-cooperative-groups-are",
    "title": "CUDA Notes: Cooperative Groups",
    "section": "What Cooperative Groups are?",
    "text": "What Cooperative Groups are?\nCooperative groups are objects that represent a collection of communicating threads. You can obtain predefined groups or partition existing groups to create subgroups. The key is that you can then call methods on these group objects, most notably sync(), to coordinate only the members of that specific group.\n// use the cooperative_groups:: namespace\nnamespace cg = cooperative_groups;"
  },
  {
    "objectID": "posts/Cooperative Groups/index.html#implicit-groups",
    "href": "posts/Cooperative Groups/index.html#implicit-groups",
    "title": "CUDA Notes: Cooperative Groups",
    "section": "Implicit Groups",
    "text": "Implicit Groups\nImplicit groups represent kernel launch configurations (threads, blocks, grids) and serve as starting points for creating specialized, hardware-accelerated groups for problem decomposition.\nSince creating implicit group handles is a collective operation requiring all threads to participate, they should be created upfront before any branching to avoid deadlocks and data corruption.\n\nThread Block\nThis is the implicit group of all the threads in the launched thread block\nImplements the same interface as thread_group\n\nstatic void sync(): synchronize the threads in the group\nstatic unsigned size(): Total number of threads in the group\nstatuc unsigned thread_rank(): Rank of the calling thread within [0, size-1]\nbool is_valid(): Whether the group violated any API costraints\n\nAdditional thread_block specific functions:\n\ndim3 group_index() 3-dimensional block index within the grid\ndim3 thread_index() 3-dimensional thraed index within the block\n\nthread_block g = this_thread_block(); // creates a group of all threads in current block\nthread_group tile32 = tiled_partition(g, 32); // partition the g into set of 32 threads (warp)\nthread_group tile4 = tiled_partition(tile32, 4); // further partition `tile32` into tiles of equally 4 threads\n\nThe tile parition size should be power of 2 and less than 1024 (this max limit varies with cc) thread_block is derived from more generic type thread_group that we used to represent tile (basically group is also a tile, check cuda c++ programming guide)\n\n\nGeneric Parallel Algorithms\nDesigning algorithms that work with any level of groups (thread block, tiles, etc)\n__device__ int reduce(thread_group g, int *x, int val)\n{\n  int lane = g.thread_rank();\n  for(int i=g.size()/2; i &gt; 0; i /=2 )\n  {\n    x[lane] = val;\n    g.sync();\n    if(lane &lt; i)\n    {\n      val += x[lane +i];\n    }\n    g.sync();\n  }\n\n  return val;\n}\n\n\n// per block\ng = this_thread_block();\nreduce(g, ptr, myVal);\n\n// per-warp level\nauto g = tiled_partition(this_thread_block(), 32);\nreduce(g, ptr, myVal);\n\n\n\nCluster Group\nThis represents all the threads launched in a single Thread Group Cluster (CC 9.0+) when a non-cluster grid is launched, the APIs assume a 1x1x1 cluster\ncluster_group g = this_cluster();\nMost of the public API is same and matches to the Thread Block implicit group, some new are as follows:\n\nnum_blocks()Total number of thread blocks in a cluster\nblock_rank()Rank of the current calling block\nthread_rank() Rank of the thread within the current calling block\nstatic unsigned int query_shared_rank(const void *addr): Obtain the block rank to which a shared memory address belongs\nstatic T* map_shared_rank(T *addr, int rank): Obtain the address of a shared memory variable of another block in the cluster\n\nnamespace cg = cooperative_groups;\n\n// ============================================================================\n// EXAMPLE 1: Basic Cluster Operations and Inter-block Communication\n// ============================================================================\n\n__global__ void cluster_basics_kernel(int *data, int *results) {\n    // Get the cluster group handle\n    cg::cluster_group cluster = cg::this_cluster();\n\n    // Basic cluster info\n    int cluster_size = cluster.num_blocks();\n    int my_block_rank = cluster.block_rank();\n    int my_thread_rank = cluster.thread_rank();\n\n    // Each block processes different data\n    int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int block_size = blockDim.x;\n\n    // Shared memory for this block\n    __shared__ int shared_data[256];\n\n    // Initialize shared memory with block-specific pattern\n    if (tid &lt; 256) {\n        shared_data[tid] = data[bid * 256 + tid];\n    }\n    __syncthreads();\n\n    // Now let's do inter-block communication within the cluster!\n    // Each block can access other blocks' shared memory\n\n    if (tid == 0) {\n        // Block 0 in cluster reads from all other blocks\n        if (my_block_rank == 0) {\n            int sum = 0;\n            for (int other_block = 0; other_block &lt; cluster_size; other_block++) {\n                // Map our shared_data address to other block's address space\n                int *other_shared = cluster.map_shared_rank(shared_data, other_block);\n\n                // Query which block owns a shared memory address\n                int owner = cluster.query_shared_rank(other_shared);\n\n                // Read first element from each block\n                sum += other_shared[0];\n            }\n            results[blockIdx.x] = sum;\n        }\n    }\n\n    // Cluster-wide barrier - all threads in all blocks wait here\n    cluster.sync();\n\n    // After sync, all blocks have completed their work\n    if (tid == 0 && my_block_rank == 0) {\n        printf(\"Cluster %d completed with %d blocks\\n\",\n               blockIdx.x / cluster_size, cluster_size);\n    }\n}\nWorking with Distributed Shared Memory is very fast as compare to traditional Global memory synchronization for inter-block communication.\n\n\nGrid Group\nA set of threads within the same grid, guranteed to be resident on the device.\ngrid_group g = this_grid();\nstatic unsigned long long thread_rank(): Rank of the calling thread within [0, num_threads)\nstatic unsigned long long block_rank(): Rank of the calling block within [0, num_blocks)\nstatic unsigned long long cluster_rank(): Rank of the calling cluster within [0, num_clusters)\nstatic unsigned long long num_threads(): Total number of threads in the group\nstatic unsigned long long num_blocks(): Total number of blocks in the group\nstatic unsigned long long num_clusters(): Total number of clusters in the group\nstatic dim3 dim_blocks(): Dimensions of the launched grid in units of blocks\nstatic dim3 dim_clusters(): Dimensions of the launched grid in units of clusters\nstatic dim3 block_index(): 3-Dimensional index of the block within the launched grid\nstatic dim3 cluster_index(): 3-Dimensional index of the cluster within the launched grid\n\nEnsure device supports the cooperative launch api\n\nint dev = 0;\nint supportsCoopLaunch = 0;\ncudaDeviceGetAttribute(&supportsCoopLaunch, cudaDevAttrCooperativeLaunch, dev);\nCUDA Occupancy and Grid-Level Synchronization Constraints:\nEach SM has limited thread capacity (e.g., 2048 threads/SM), giving a total system capacity of num_SMs × threads_per_SM. While launching many threads seems beneficial, CUDA schedules thread blocks in streaming fashion as SMs become available, so excessive thread launches don’t improve performance due to occupancy limits. Grid-stride loops are more efficient as they reuse threads across data portions instead of creating thread explosion.\nThe critical issue arises with grid-level synchronization: if all active SMs hit a synchronization barrier while other thread blocks wait in the scheduling queue, those waiting blocks cannot proceed, creating deadlock. This means grid size cannot exceed occupancy-determined limits for cooperative kernels.\nOccupancy calculations answer “how many threads can actually execute simultaneously per SM based on kernel design,” which determines the maximum safe grid size formula: max_safe_grid_size = num_SMs × actual_threads_per_SM_for_this_kernel. This occupancy-based upper bound is essential for any kernel requiring grid-level synchronization to prevent deadlock scenarios.\n\nCUDA provides the API to fetch number of concurrent thread blocks per SM. cudaOccupancyMaxActiveBlocksPerMultiprocessor\n// Device code\n__global__ void MyKernel(int *d, int *a, int *b)\n{\n    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    d[idx] = a[idx] * b[idx];\n}\n\n// Host code\nint main()\n{\n    int numBlocks;        // Occupancy in terms of active blocks\n    int blockSize = 32;\n\n    // These variables are used to convert occupancy to warps\n    int device;\n    cudaDeviceProp prop;\n    int activeWarps;\n    int maxWarps;\n\n    cudaGetDevice(&device);\n    cudaGetDeviceProperties(&prop, device);\n\n    cudaOccupancyMaxActiveBlocksPerMultiprocessor(\n        &numBlocks,\n        MyKernel,\n        blockSize,\n        0);\n\n    activeWarps = numBlocks * blockSize / prop.warpSize;\n    maxWarps = prop.maxThreadsPerMultiProcessor / prop.warpSize;\n\n    std::cout &lt;&lt; \"Occupancy: \" &lt;&lt; (double)activeWarps / maxWarps * 100 &lt;&lt; \"%\" &lt;&lt; std::endl;\n\n    return 0;\n}\n\nTotal Number of blocks that can be launched concurrently in a grid can be total_no_of_SM * numBlocks this is a requirement for cooperative grid launch.\nGrid-Stride loop methodology works nicely with this requirement.\n\n__global__ void kernel(...)\n{\n    grid_group grid = this_grid();\n    // load data\n    // loop - compute, share data\n    grid.sync(); // device wide execution barrier\n}\n\nThis kernel should be launched via cudaLaunchCooperativeKernel(...) instead of &lt;&lt;&lt;...&gt;&gt;&gt; (three chevron)\n\n/// This will launch a grid that can maximally fill the GPU, on the default stream with kernel arguments\nint numBlocksPerSm = 0;\n // Number of threads my_kernel will be launched with\nint numThreads = 128;\ncudaDeviceProp deviceProp;\ncudaGetDeviceProperties(&deviceProp, dev);\ncudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlocksPerSm, my_kernel, numThreads, 0);\n// launch\nvoid *kernelArgs[] = { /* add kernel args */ };\ndim3 dimBlock(numThreads, 1, 1);\ndim3 dimGrid(deviceProp.multiProcessorCount*numBlocksPerSm, 1, 1);\ncudaLaunchCooperativeKernel((void*)my_kernel, dimGrid, dimBlock, kernelArgs);\n\n\nMotivation\nSuch fine level access allow us to perform specialization tasks, for example I can make my one thread-block to do something and other to do something else (thread block level specialization)\nWarp Specialization Identify the id of each unique warp and one does something than other. These ideas help in developing algorithms that work like Master-Slave or Producer Consumer. Which in turns maps the idea of Persistent Kernels (kernel which stays for long period)\nFor example: A master entity in my kernel might be looking at the queue only and then distributing the queue items to the slave entities in my kernel (which process them) to another end of queue it can be host of another GPU. And this also gives a good motivation to why we would be needing the grid wide sync\nAnother reason we might want to have persistent kernels because of the re-usage of kernel state (shared memory, registers, etc)"
  },
  {
    "objectID": "posts/Cooperative Groups/index.html#explicit-group",
    "href": "posts/Cooperative Groups/index.html#explicit-group",
    "title": "CUDA Notes: Cooperative Groups",
    "section": "Explicit Group",
    "text": "Explicit Group\n\nThread Block Tile\nA subset of thread blockm divided into tiles in row major order. This is another way of creating them in tempalte manner, but the size should be known at compile time contants (This provides more functionality) + fast (size known at compile time)\nthread_block_tile&lt;32&gt; tile32 = tiled_partition&lt;32&gt;(this_thread_block());\nthread_block_tile&lt;4&gt; tile4 = tiled_partition&lt;4&gt;(this_thread_block());\nthread_block_tile&lt;1&gt; this_thread(); // single threaded group\nAdditional functionality (but on group level)\n\nSimilar to warp shuffle primitives\n\nshfl()\nshfl_down()\nshfl_up()\nshfl_xor()\n\nSimilar to warp voting primitives (check cuda programming guide)\n\nany()\nall()\nballot()\n\nSimilr to warp-matching primitives\n\nmatch_any()\nmatch_all()\n\n\ntemplate &lt;unsigned size&gt;\n__device__ int tile_reduce(thread_block_tile&lt;size&gt;, int val)\n{\n  for(int i=g.size()/2; i &gt; 0; i /=2 )\n  {\n    val += g.shfl_down(val, i); // directly summing to the val of next (i + threads) in group\n  }\n  return val;\n}\n\n\n// per tile of 16 threads\nthread_block_tile&lt;16&gt; g = tiled_partition&lt;16&gt;(this_thread_block());\ntile_reduce(g, myVal);\n\n\nCoalesced Groups\nDiscover a set of coalesced threads, i.e. a group of converged threads executing in SIMD\nIn CUDA’s SIMT architecture, at the hardware level the multiprocessor executes threads in groups of 32 called warps. If there exists a data-dependent conditional branch in the application code such that threads within a warp diverge, then the warp serially executes each branch disabling threads not on that path. The threads that remain active on the path are referred to as coalesced. Cooperative Groups has functionality to discover, and create, a group containing all coalesced threads.\ncoalesced_group active = coalesced_threads();\n\n// Example: Atomic Aggregation\n\ninline __device__ int atomicAggInc(int *p)\n{\n    coalesced_group g = coalesced_threads();\n    int pre;\n    if(g.thread_rank() == 0)\n        prev = atomicAdd(p, g.size());\n    prev = g.thread_rank() + g.shfl(prev, 0); // Note: this g.shfl is actually __shfl_sync's alias\n    return prev;\n}\n\nApart fromt the above content there are synchronization and other primitives that I am avoiding in this article, as they are intuitively simple and can be read/reference from the CUDA programming guide."
  },
  {
    "objectID": "posts/The Transparent Algorithm/index.html#table-of-content",
    "href": "posts/The Transparent Algorithm/index.html#table-of-content",
    "title": "[WIP] The Transparent Algorithm",
    "section": "Table of Content",
    "text": "Table of Content\n\n\n\n#\nTitle\nDescription\n\n\n\n\n1\nThe Mathematics of Learning Guarantees\nFoundations of learning theory, error bounds, and the concept of misleading samples"
  },
  {
    "objectID": "posts/Information-Theory-In-ML/index.html#the-frequentist-approach",
    "href": "posts/Information-Theory-In-ML/index.html#the-frequentist-approach",
    "title": "Information Theory in Machine Learning: A Fun Approach",
    "section": "The Frequentist Approach",
    "text": "The Frequentist Approach\nMost people would say “Easy! It’s 95%!” After all, that’s the test’s accuracy, right?\nLet’s try the traditional frequency-based solution:\n\nRun the test on 1000 people\nCount how many positive results were correct\nDivide by total positive results\nThat’s our probability!\n\nBut wait… there’s a catch. The disease is rare, affecting only 1 in 1000 people. Now our frequency calculations get interesting:\n\nIn 1000 people, only 1 person actually has the disease\nWith 95% accuracy, the test will correctly identify this 1 person\nBut it will also incorrectly flag about 50 healthy people (5% of 999)\nSo out of 51 positive results, only 1 is correct!\n\nThe actual probability is closer to 2%, not 95%. Mind blown yet?"
  },
  {
    "objectID": "posts/Information-Theory-In-ML/index.html#when-frequencies-fail",
    "href": "posts/Information-Theory-In-ML/index.html#when-frequencies-fail",
    "title": "Information Theory in Machine Learning: A Fun Approach",
    "section": "When Frequencies Fail",
    "text": "When Frequencies Fail\nBut here’s the real problem - we can’t actually run this experiment 1000 times for our current patient. We have:\n\nOne patient\nOne test result\nOne decision to make\n\nWe need a way to reason about this specific case, using everything we know."
  },
  {
    "objectID": "posts/Information-Theory-In-ML/index.html#enter-bayesian-thinking",
    "href": "posts/Information-Theory-In-ML/index.html#enter-bayesian-thinking",
    "title": "Information Theory in Machine Learning: A Fun Approach",
    "section": "Enter Bayesian Thinking",
    "text": "Enter Bayesian Thinking\nThis is where Bayesian probability shines. Instead of counting frequencies, we think about:\n\nWhat we knew before the test (prior)\nWhat the test tells us (evidence)\nHow to combine these (update)\n\nFor our patient:\n\nPrior: 1/1000 chance of disease (population rate)\nEvidence: Positive test result (95% accurate)\nUpdate: Combine these mathematically (we’ll show how!)"
  },
  {
    "objectID": "posts/Information-Theory-In-ML/index.html#the-bayesian-formula",
    "href": "posts/Information-Theory-In-ML/index.html#the-bayesian-formula",
    "title": "Information Theory in Machine Learning: A Fun Approach",
    "section": "The Bayesian Formula",
    "text": "The Bayesian Formula\nThis thinking gives us the famous Bayes’ Theorem: \\[\nP(Disease|Positive) = \\frac{P(Positive|Disease) \\times P(Disease)}{P(Positive)}\n\\]\nWhere:\n\n\\(P(Disease|Positive)\\) is what we want: probability of disease given a positive test\n\\(P(Positive|Disease)\\) is 0.95 (test sensitivity)\n\\(P(Disease)\\) is 1/1000 (disease prevalence)\n\\(P(Positive)\\) needs to be calculated using total probability\n\nTo find \\(P(Positive)\\), we consider both ways to get a positive result:\n\nHaving the disease and testing positive\nNot having the disease but testing positive anyway\n\nMathematically: \\(P(Positive) = P(Positive|Disease)P(Disease) + P(Positive|No Disease)P(No Disease)\\)\nPlugging in our numbers: \\(P(Positive) = 0.95 \\times (1/1000) + 0.05 \\times (999/1000)\\) = 0.0509\nNow we can solve our original equation:\n\\[\nP(Disease|Positive) = \\frac{0.95 \\times (1/1000)}{0.0509} \\approx 0.019\n\\]\nThat’s about 2%! Even with a “95% accurate” test, a positive result only means a 2% chance of actually having the disease. This surprising result comes from combining three pieces of information:\n\nHow rare the disease is (prior probability)\nHow accurate the test is (sensitivity)\nHow often it gives false alarms (specificity)\n\nModern ML models work in a similar way. When a model says “I’m 90% confident this is a cat’s image”, it’s not just matching pixels - it’s combining prior knowledge about what cats look like with the specific evidence in the image. Just like our medical test, it’s all about updating beliefs with evidence!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Swayam’s Scripts",
    "section": "",
    "text": "[WIP] Effective C++ (Complete series)\n\n\nThis article summarizes all the items mentioned in Effective C++ (2005), More Effective C++ (1995) and Effective Modern C++ (2015) by Scott Meyers\n\n\n\nC++\n\n\n\n\n\n\n\n\n\nSep 19, 2025\n\n\nSwayam Singh\n\n16 min\n\n3,077 words\n\n\n\n\n\n\n\n\n\n\n\n\nCUDA Notes: Cooperative Groups\n\n\nBeyond Block Barriers, Flexible Thread Synchronization from Warps to Grids\n\n\n\nCUDA\n\n\n\n\n\n\n\n\n\nAug 15, 2025\n\n\nSwayam Singh\n\n13 min\n\n2,508 words\n\n\n\n\n\n\n\n\n\n\n\n\n[WIP] A Hitchhiker’s Guide to LLVM\n\n\nJust Another Day in the Life of a SSA Variable\n\n\n\nCompilers\n\nLLVM\n\n\n\n\n\n\n\n\n\nJun 3, 2025\n\n\nSwayam Singh\n\n9 min\n\n1,796 words\n\n\n\n\n\n\n\n\n\n\n\n\nGradient Flow and Variance Propogation Analysis of Dynamic Tanh Layer\n\n\nA Mathematical Investigation into DyT’s Potential to Mitigate the Curse of Depth in Pre-LN Transformers\n\n\n\nML-Theory\n\nTransformers\n\n\n\n\n\n\n\n\n\nMar 21, 2025\n\n\nSwayam Singh\n\n9 min\n\n1,625 words\n\n\n\n\n\n\n\n\n\n\n\n\n[WIP] The Transparent Algorithm\n\n\nA bridge between formal mathematical proofs and intuitive understanding, guiding readers through the theoretical foundations of machine learning with clarity and precision.\n\n\n\nML\n\nTheory\n\nBook\n\n\n\n\n\n\n\n\n\nMar 15, 2025\n\n\nSwayam Singh\n\n1 min\n\n35 words\n\n\n\n\n\n\n\n\n\n\n\n\nConcurrent C++ A Practical Guide\n\n\nA concise and practical guide to mastering C++ concurrency, covering threads, synchronization, parallelism, and debugging—all in one place.\n\n\n\nC++\n\nBook\n\n\n\n\n\n\n\n\n\nFeb 9, 2025\n\n\nSwayam Singh\n\n1 min\n\n5 words\n\n\n\n\n\n\n\n\n\n\n\n\nInformation Theory in Machine Learning: A Fun Approach\n\n\nTeaching Neural Networks to Handle Their Trust Issues\n\n\n\nML\n\nInformation-Theory\n\n\n\n\n\n\n\n\n\nJan 9, 2025\n\n\nSwayam Singh\n\n14 min\n\n2,650 words\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Perplexity\n\n\nA New Perspective on Model Uncertainty\n\n\n\nLLM\n\n\n\n\n\n\n\n\n\nOct 10, 2024\n\n\nSwayam Singh\n\n3 min\n\n595 words\n\n\n\n\n\n\n\n\n\n\n\n\nNumpy QuadDType: Quadruple Precision for Everyone\n\n\nQuad Precision for All: Simplifying High-Accuracy Computing with numpy_quaddtype\n\n\n\nNumPy\n\nOpen-Source\n\n\n\n\n\n\n\n\n\nSep 30, 2024\n\n\nSwayam Singh\n\n1 min\n\n5 words\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Attention Mimicking Gradient Descent\n\n\n\n\n\n\nNLP\n\nTransformers\n\n\n\n\n\n\n\n\n\nOct 14, 2023\n\n\nSwayam Singh\n\n9 min\n\n1,671 words\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/The Transparent Algorithm/chapter-1.html",
    "href": "pages/The Transparent Algorithm/chapter-1.html",
    "title": "Chapter 1: The Mathematics of Learning Guarantees",
    "section": "",
    "text": "When a machine learning algorithm examines data and produces a model, how can we be confident that this model will perform well on new, unseen examples? This fundamental question lies at the heart of learning theory. Rather than focusing on specific algorithms like neural networks or decision trees, learning theory provides a mathematical framework to understand when and why learning works.\nConsider a simple problem: we want to determine if an email is spam based on its content. We might train a classifier on thousands of labeled examples, but how do we know if it will work well on future emails? Intuitively, we expect that with enough diverse training examples, our classifier should generalize well to new data. But how many examples are “enough”? And how can we formally prove this intuition?\nLearning theory gives us tools to answer these questions with mathematical precision. Instead of vague assurances, we can derive exact bounds on how likely our algorithm is to succeed and how much data it needs. This chapter introduces the fundamental concepts needed to build such guarantees.\n\n\n\nTo reason precisely about learning, we need formal definitions. Let’s establish our notation:\n\nInput space \\(\\mathcal{X}\\): The set of all possible inputs (e.g., all possible emails)\nOutput space \\(\\mathcal{Y}\\): The set of all possible outputs (e.g., “spam” or “not spam”)\nTraining sample \\(S = ((x_1, y_1), (x_2, y_2), ..., (x_m, y_m))\\): A set of m labeled examples\n\\(S|_x = (x_1, x_2, ..., x_m)\\): Just the inputs from the training sample\nDistribution \\(\\mathcal{D}\\): The probability distribution from which examples are drawn\nLabeling function \\(f: \\mathcal{X} \\rightarrow \\mathcal{Y}\\): The true, underlying function we’re trying to learn\nHypothesis class \\(\\mathcal{H}\\): The set of possible prediction rules our algorithm can output\nHypothesis \\(h \\in \\mathcal{H}\\): A specific prediction rule (e.g., a specific classifier)\n\nGiven these definitions, we can formalize two crucial concepts:\n\nTrue error: The probability that a hypothesis makes an incorrect prediction on an example drawn from the true distribution: \\[L_{(D,f)}(h) = \\mathbb{P}_{x \\sim D}[h(x) \\neq f(x)]\\]\nTraining error: The fraction of mistakes a hypothesis makes on the training sample: \\[L_S(h) = \\frac{1}{m}\\sum_{i=1}^{m} \\mathbb{1}_{[h(x_i) \\neq y_i]}\\] where \\(\\mathbb{1}\\) is the indicator function that equals 1 when \\(h(x_i) \\neq y_i\\) and 0 otherwise.\n\nOur learning algorithm examines the training data \\(S\\) and outputs a hypothesis \\(h_S\\). We want \\(h_S\\) to have low true error, but we can only measure its training error. The key challenge in learning theory is understanding the relationship between these two errors.\nFor simplicity, we’ll initially work under the realizability assumption: there exists at least one hypothesis \\(h^* \\in \\mathcal{H}\\) with zero true error (\\(L_{(D,f)}(h^*) = 0\\)). This means our hypothesis class contains at least one perfect predictor.\n\n\n\nWhen does a learning algorithm fail? We define failure as producing a hypothesis with true error exceeding some acceptable threshold \\(\\epsilon\\):\n\\[L_{(D,f)}(h_S) &gt; \\epsilon\\]\nOur goal is to understand how likely this failure is when we train on \\(m\\) randomly drawn examples.\nTo approach this systematically, let’s define the set of “bad” hypotheses:\n\\[\\mathcal{H}_B = \\{h \\in \\mathcal{H} : L_{(D,f)}(h) &gt; \\epsilon\\}\\]\nThese are hypotheses that perform poorly on the true distribution. If our algorithm outputs such a hypothesis, it fails.\nBut here’s the key insight: under the realizability assumption, our algorithm will always output a hypothesis with zero training error. So failure can only happen if some “bad” hypothesis also happens to have zero training error on our sample.\nThis leads us to define the set of “misleading samples”:\n\\[M = \\{S|_x : \\exists h \\in \\mathcal{H}_B, L_S(h) = 0\\}\\]\nThese are training sets where at least one bad hypothesis looks perfect (has zero training error). Since our algorithm chooses a hypothesis with zero training error, if a bad hypothesis has zero training error, our algorithm might select it and fail.\nThe crucial relationship is: \\[\\{S|_x : L_{(D,f)}(h_S) &gt; \\epsilon\\} \\subseteq M\\]\nEvery sample that leads to algorithm failure must be a misleading sample. This insight allows us to bound the probability of failure by analyzing the probability of drawing a misleading sample.\n\n\n\nNow we can calculate the probability of drawing a misleading sample. First, we express \\(M\\) as a union:\n\\[M = \\bigcup_{h \\in \\mathcal{H}_B} \\{S|_x : L_S(h) = 0\\}\\]\nThis allows us to apply the union bound:\n\\[\\mathcal{D}^m(\\{S|_x : L_{(D,f)}(h_S) &gt; \\epsilon\\}) \\leq \\mathcal{D}^m(M) \\leq \\sum_{h \\in \\mathcal{H}_B} \\mathcal{D}^m(\\{S|_x : L_S(h) = 0\\})\\]\nNext, we calculate the probability that a specific bad hypothesis \\(h\\) has zero training error. Since the true error of \\(h\\) is greater than \\(\\epsilon\\), and our samples are drawn independently:\n\\[\\mathcal{D}^m(\\{S|_x : L_S(h) = 0\\}) = \\mathcal{D}^m(\\{S|_x : \\forall i, h(x_i) = f(x_i)\\}) = \\prod_{i=1}^m \\mathcal{D}(\\{x_i : h(x_i) = f(x_i)\\})\\]\nFor each individual sample, the probability of correct classification is:\n\\[\\mathcal{D}(\\{x_i : h(x_i) = f(x_i)\\}) = 1 - L_{(D,f)}(h) \\leq 1 - \\epsilon\\]\nTherefore: \\[\\mathcal{D}^m(\\{S|_x : L_S(h) = 0\\}) \\leq (1 - \\epsilon)^m\\]\nTo simplify our expressions, we use the standard inequality \\(1 - \\epsilon \\leq e^{-\\epsilon}\\), which gives us:\n\\[\\mathcal{D}^m(\\{S|_x : L_S(h) = 0\\}) \\leq (1 - \\epsilon)^m \\leq e^{-\\epsilon m}\\]\nCombining this with our union bound:\n\\[\\mathcal{D}^m(\\{S|_x : L_{(D,f)}(h_S) &gt; \\epsilon\\}) \\leq \\sum_{h \\in \\mathcal{H}_B} e^{-\\epsilon m} = |\\mathcal{H}_B| \\cdot e^{-\\epsilon m} \\leq |\\mathcal{H}| \\cdot e^{-\\epsilon m}\\]\nThis gives us our fundamental bound: the probability of failure is at most \\(|\\mathcal{H}| \\cdot e^{-\\epsilon m}\\).\n\n\n\nThe bound we’ve derived forms the foundation of Probably Approximately Correct (PAC) learning. Let’s understand what it means in practical terms.\nIf we want to ensure that our algorithm fails with probability at most \\(\\delta\\) (a confidence parameter), we need:\n\\[|\\mathcal{H}| \\cdot e^{-\\epsilon m} \\leq \\delta\\]\nSolving for the required sample size \\(m\\):\n\\[e^{-\\epsilon m} \\leq \\frac{\\delta}{|\\mathcal{H}|}\\]\nTaking logarithms:\n\\[-\\epsilon m \\leq \\ln\\left(\\frac{\\delta}{|\\mathcal{H}|}\\right) = \\ln(\\delta) - \\ln(|\\mathcal{H}|)\\]\nDividing by \\(-\\epsilon\\) and flipping the inequality:\n\\[m \\geq \\frac{\\ln(|\\mathcal{H}|) - \\ln(\\delta)}{\\epsilon} = \\frac{\\ln(|\\mathcal{H}|/\\delta)}{\\epsilon}\\]\nThis gives us the PAC learning guarantee: With probability at least \\(1-\\delta\\), a learning algorithm that outputs a hypothesis with zero training error will have true error at most \\(\\epsilon\\), provided the training sample size is at least:\n\\[m \\geq \\frac{\\ln(|\\mathcal{H}|/\\delta)}{\\epsilon}\\]\nThis is a fundamental result that tells us exactly how much data we need to learn effectively. It shows that the sample complexity: - Scales logarithmically with the size of the hypothesis class (\\(|\\mathcal{H}|\\)) - Scales logarithmically with the inverse of the confidence parameter (\\(1/\\delta\\)) - Scales linearly with the inverse of the error threshold (\\(1/\\epsilon\\))\nThis means that we can learn even from very large hypothesis classes with a reasonable amount of data, as long as we’re willing to accept a small probability of failure and a small error threshold.\n\nWe can make the similar interpretation from the experiment shown in the image.\n\n\n\nIn this chapter, we developed the fundamental mathematics that allow us to provide guarantees about learning algorithms. Rather than relying on intuition or empirical evidence alone, we now have precise tools to analyze when and why learning succeeds.\nLet’s review the key insights we’ve gained:\n\nWe formalized the learning problem through precise definitions of hypotheses, errors, and learning algorithms.\nWe identified that learning failure occurs when our algorithm outputs a hypothesis with high true error.\nWe discovered that such failure can only happen when our training sample is “misleading” - making a bad hypothesis look perfect.\nWe derived tight bounds on the probability of drawing misleading samples, leading to our fundamental inequality: \\[\\mathcal{D}^m(\\{S|_x : L_{(D,f)}(h_S) &gt; \\epsilon\\}) \\leq |\\mathcal{H}| \\cdot e^{-\\epsilon m}\\]\nWe established the sample complexity of learning: \\[m \\geq \\frac{\\ln(|\\mathcal{H}|/\\delta)}{\\epsilon}\\] This tells us exactly how many examples we need to ensure our learning algorithm succeeds with high probability.\n\nThese results provide a solid mathematical foundation for understanding learning guarantees, but they rely on several assumptions that limit their direct application to many real-world scenarios. Most notably, we’ve worked under the realizability assumption (that a perfect hypothesis exists in our class), and we’ve assumed that our hypothesis class is finite.\nIn the next chapter, we’ll formalize and extend these insights by developing the Probably Approximately Correct (PAC) learning model. This model provides a comprehensive framework for analyzing learnability across different problem domains and algorithm types. We’ll remove the restrictive assumptions of this chapter, explore how to handle infinite hypothesis classes, and investigate learning without the realizability assumption.\nBy building on the mathematical foundations established here, we’ll develop a more complete understanding of what makes learning possible and what fundamental limitations exist in machine learning. This will ultimately help us answer one of the most profound questions in the field: What is learnable and what is not?"
  },
  {
    "objectID": "pages/The Transparent Algorithm/chapter-1.html#introduction-to-learning-theory",
    "href": "pages/The Transparent Algorithm/chapter-1.html#introduction-to-learning-theory",
    "title": "Chapter 1: The Mathematics of Learning Guarantees",
    "section": "",
    "text": "When a machine learning algorithm examines data and produces a model, how can we be confident that this model will perform well on new, unseen examples? This fundamental question lies at the heart of learning theory. Rather than focusing on specific algorithms like neural networks or decision trees, learning theory provides a mathematical framework to understand when and why learning works.\nConsider a simple problem: we want to determine if an email is spam based on its content. We might train a classifier on thousands of labeled examples, but how do we know if it will work well on future emails? Intuitively, we expect that with enough diverse training examples, our classifier should generalize well to new data. But how many examples are “enough”? And how can we formally prove this intuition?\nLearning theory gives us tools to answer these questions with mathematical precision. Instead of vague assurances, we can derive exact bounds on how likely our algorithm is to succeed and how much data it needs. This chapter introduces the fundamental concepts needed to build such guarantees."
  },
  {
    "objectID": "pages/The Transparent Algorithm/chapter-1.html#key-notation-and-definitions",
    "href": "pages/The Transparent Algorithm/chapter-1.html#key-notation-and-definitions",
    "title": "Chapter 1: The Mathematics of Learning Guarantees",
    "section": "",
    "text": "To reason precisely about learning, we need formal definitions. Let’s establish our notation:\n\nInput space \\(\\mathcal{X}\\): The set of all possible inputs (e.g., all possible emails)\nOutput space \\(\\mathcal{Y}\\): The set of all possible outputs (e.g., “spam” or “not spam”)\nTraining sample \\(S = ((x_1, y_1), (x_2, y_2), ..., (x_m, y_m))\\): A set of m labeled examples\n\\(S|_x = (x_1, x_2, ..., x_m)\\): Just the inputs from the training sample\nDistribution \\(\\mathcal{D}\\): The probability distribution from which examples are drawn\nLabeling function \\(f: \\mathcal{X} \\rightarrow \\mathcal{Y}\\): The true, underlying function we’re trying to learn\nHypothesis class \\(\\mathcal{H}\\): The set of possible prediction rules our algorithm can output\nHypothesis \\(h \\in \\mathcal{H}\\): A specific prediction rule (e.g., a specific classifier)\n\nGiven these definitions, we can formalize two crucial concepts:\n\nTrue error: The probability that a hypothesis makes an incorrect prediction on an example drawn from the true distribution: \\[L_{(D,f)}(h) = \\mathbb{P}_{x \\sim D}[h(x) \\neq f(x)]\\]\nTraining error: The fraction of mistakes a hypothesis makes on the training sample: \\[L_S(h) = \\frac{1}{m}\\sum_{i=1}^{m} \\mathbb{1}_{[h(x_i) \\neq y_i]}\\] where \\(\\mathbb{1}\\) is the indicator function that equals 1 when \\(h(x_i) \\neq y_i\\) and 0 otherwise.\n\nOur learning algorithm examines the training data \\(S\\) and outputs a hypothesis \\(h_S\\). We want \\(h_S\\) to have low true error, but we can only measure its training error. The key challenge in learning theory is understanding the relationship between these two errors.\nFor simplicity, we’ll initially work under the realizability assumption: there exists at least one hypothesis \\(h^* \\in \\mathcal{H}\\) with zero true error (\\(L_{(D,f)}(h^*) = 0\\)). This means our hypothesis class contains at least one perfect predictor."
  },
  {
    "objectID": "pages/The Transparent Algorithm/chapter-1.html#understanding-failure-events",
    "href": "pages/The Transparent Algorithm/chapter-1.html#understanding-failure-events",
    "title": "Chapter 1: The Mathematics of Learning Guarantees",
    "section": "",
    "text": "When does a learning algorithm fail? We define failure as producing a hypothesis with true error exceeding some acceptable threshold \\(\\epsilon\\):\n\\[L_{(D,f)}(h_S) &gt; \\epsilon\\]\nOur goal is to understand how likely this failure is when we train on \\(m\\) randomly drawn examples.\nTo approach this systematically, let’s define the set of “bad” hypotheses:\n\\[\\mathcal{H}_B = \\{h \\in \\mathcal{H} : L_{(D,f)}(h) &gt; \\epsilon\\}\\]\nThese are hypotheses that perform poorly on the true distribution. If our algorithm outputs such a hypothesis, it fails.\nBut here’s the key insight: under the realizability assumption, our algorithm will always output a hypothesis with zero training error. So failure can only happen if some “bad” hypothesis also happens to have zero training error on our sample.\nThis leads us to define the set of “misleading samples”:\n\\[M = \\{S|_x : \\exists h \\in \\mathcal{H}_B, L_S(h) = 0\\}\\]\nThese are training sets where at least one bad hypothesis looks perfect (has zero training error). Since our algorithm chooses a hypothesis with zero training error, if a bad hypothesis has zero training error, our algorithm might select it and fail.\nThe crucial relationship is: \\[\\{S|_x : L_{(D,f)}(h_S) &gt; \\epsilon\\} \\subseteq M\\]\nEvery sample that leads to algorithm failure must be a misleading sample. This insight allows us to bound the probability of failure by analyzing the probability of drawing a misleading sample."
  },
  {
    "objectID": "pages/The Transparent Algorithm/chapter-1.html#probabilistic-error-bounds",
    "href": "pages/The Transparent Algorithm/chapter-1.html#probabilistic-error-bounds",
    "title": "Chapter 1: The Mathematics of Learning Guarantees",
    "section": "",
    "text": "Now we can calculate the probability of drawing a misleading sample. First, we express \\(M\\) as a union:\n\\[M = \\bigcup_{h \\in \\mathcal{H}_B} \\{S|_x : L_S(h) = 0\\}\\]\nThis allows us to apply the union bound:\n\\[\\mathcal{D}^m(\\{S|_x : L_{(D,f)}(h_S) &gt; \\epsilon\\}) \\leq \\mathcal{D}^m(M) \\leq \\sum_{h \\in \\mathcal{H}_B} \\mathcal{D}^m(\\{S|_x : L_S(h) = 0\\})\\]\nNext, we calculate the probability that a specific bad hypothesis \\(h\\) has zero training error. Since the true error of \\(h\\) is greater than \\(\\epsilon\\), and our samples are drawn independently:\n\\[\\mathcal{D}^m(\\{S|_x : L_S(h) = 0\\}) = \\mathcal{D}^m(\\{S|_x : \\forall i, h(x_i) = f(x_i)\\}) = \\prod_{i=1}^m \\mathcal{D}(\\{x_i : h(x_i) = f(x_i)\\})\\]\nFor each individual sample, the probability of correct classification is:\n\\[\\mathcal{D}(\\{x_i : h(x_i) = f(x_i)\\}) = 1 - L_{(D,f)}(h) \\leq 1 - \\epsilon\\]\nTherefore: \\[\\mathcal{D}^m(\\{S|_x : L_S(h) = 0\\}) \\leq (1 - \\epsilon)^m\\]\nTo simplify our expressions, we use the standard inequality \\(1 - \\epsilon \\leq e^{-\\epsilon}\\), which gives us:\n\\[\\mathcal{D}^m(\\{S|_x : L_S(h) = 0\\}) \\leq (1 - \\epsilon)^m \\leq e^{-\\epsilon m}\\]\nCombining this with our union bound:\n\\[\\mathcal{D}^m(\\{S|_x : L_{(D,f)}(h_S) &gt; \\epsilon\\}) \\leq \\sum_{h \\in \\mathcal{H}_B} e^{-\\epsilon m} = |\\mathcal{H}_B| \\cdot e^{-\\epsilon m} \\leq |\\mathcal{H}| \\cdot e^{-\\epsilon m}\\]\nThis gives us our fundamental bound: the probability of failure is at most \\(|\\mathcal{H}| \\cdot e^{-\\epsilon m}\\)."
  },
  {
    "objectID": "pages/The Transparent Algorithm/chapter-1.html#the-probably-approximately-correct-framework",
    "href": "pages/The Transparent Algorithm/chapter-1.html#the-probably-approximately-correct-framework",
    "title": "Chapter 1: The Mathematics of Learning Guarantees",
    "section": "",
    "text": "The bound we’ve derived forms the foundation of Probably Approximately Correct (PAC) learning. Let’s understand what it means in practical terms.\nIf we want to ensure that our algorithm fails with probability at most \\(\\delta\\) (a confidence parameter), we need:\n\\[|\\mathcal{H}| \\cdot e^{-\\epsilon m} \\leq \\delta\\]\nSolving for the required sample size \\(m\\):\n\\[e^{-\\epsilon m} \\leq \\frac{\\delta}{|\\mathcal{H}|}\\]\nTaking logarithms:\n\\[-\\epsilon m \\leq \\ln\\left(\\frac{\\delta}{|\\mathcal{H}|}\\right) = \\ln(\\delta) - \\ln(|\\mathcal{H}|)\\]\nDividing by \\(-\\epsilon\\) and flipping the inequality:\n\\[m \\geq \\frac{\\ln(|\\mathcal{H}|) - \\ln(\\delta)}{\\epsilon} = \\frac{\\ln(|\\mathcal{H}|/\\delta)}{\\epsilon}\\]\nThis gives us the PAC learning guarantee: With probability at least \\(1-\\delta\\), a learning algorithm that outputs a hypothesis with zero training error will have true error at most \\(\\epsilon\\), provided the training sample size is at least:\n\\[m \\geq \\frac{\\ln(|\\mathcal{H}|/\\delta)}{\\epsilon}\\]\nThis is a fundamental result that tells us exactly how much data we need to learn effectively. It shows that the sample complexity: - Scales logarithmically with the size of the hypothesis class (\\(|\\mathcal{H}|\\)) - Scales logarithmically with the inverse of the confidence parameter (\\(1/\\delta\\)) - Scales linearly with the inverse of the error threshold (\\(1/\\epsilon\\))\nThis means that we can learn even from very large hypothesis classes with a reasonable amount of data, as long as we’re willing to accept a small probability of failure and a small error threshold.\n\nWe can make the similar interpretation from the experiment shown in the image."
  },
  {
    "objectID": "pages/The Transparent Algorithm/chapter-1.html#summary-and-looking-ahead",
    "href": "pages/The Transparent Algorithm/chapter-1.html#summary-and-looking-ahead",
    "title": "Chapter 1: The Mathematics of Learning Guarantees",
    "section": "",
    "text": "In this chapter, we developed the fundamental mathematics that allow us to provide guarantees about learning algorithms. Rather than relying on intuition or empirical evidence alone, we now have precise tools to analyze when and why learning succeeds.\nLet’s review the key insights we’ve gained:\n\nWe formalized the learning problem through precise definitions of hypotheses, errors, and learning algorithms.\nWe identified that learning failure occurs when our algorithm outputs a hypothesis with high true error.\nWe discovered that such failure can only happen when our training sample is “misleading” - making a bad hypothesis look perfect.\nWe derived tight bounds on the probability of drawing misleading samples, leading to our fundamental inequality: \\[\\mathcal{D}^m(\\{S|_x : L_{(D,f)}(h_S) &gt; \\epsilon\\}) \\leq |\\mathcal{H}| \\cdot e^{-\\epsilon m}\\]\nWe established the sample complexity of learning: \\[m \\geq \\frac{\\ln(|\\mathcal{H}|/\\delta)}{\\epsilon}\\] This tells us exactly how many examples we need to ensure our learning algorithm succeeds with high probability.\n\nThese results provide a solid mathematical foundation for understanding learning guarantees, but they rely on several assumptions that limit their direct application to many real-world scenarios. Most notably, we’ve worked under the realizability assumption (that a perfect hypothesis exists in our class), and we’ve assumed that our hypothesis class is finite.\nIn the next chapter, we’ll formalize and extend these insights by developing the Probably Approximately Correct (PAC) learning model. This model provides a comprehensive framework for analyzing learnability across different problem domains and algorithm types. We’ll remove the restrictive assumptions of this chapter, explore how to handle infinite hypothesis classes, and investigate learning without the realizability assumption.\nBy building on the mathematical foundations established here, we’ll develop a more complete understanding of what makes learning possible and what fundamental limitations exist in machine learning. This will ultimately help us answer one of the most profound questions in the field: What is learnable and what is not?"
  }
]