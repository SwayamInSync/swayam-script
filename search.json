[
  {
    "objectID": "posts/Information-Theory-In-ML/index.html#the-frequentist-approach",
    "href": "posts/Information-Theory-In-ML/index.html#the-frequentist-approach",
    "title": "Information Theory in Machine Learning: A Fun Approach",
    "section": "The Frequentist Approach",
    "text": "The Frequentist Approach\nMost people would say “Easy! It’s 95%!” After all, that’s the test’s accuracy, right?\nLet’s try the traditional frequency-based solution:\n\nRun the test on 1000 people\nCount how many positive results were correct\nDivide by total positive results\nThat’s our probability!\n\nBut wait… there’s a catch. The disease is rare, affecting only 1 in 1000 people. Now our frequency calculations get interesting:\n\nIn 1000 people, only 1 person actually has the disease\nWith 95% accuracy, the test will correctly identify this 1 person\nBut it will also incorrectly flag about 50 healthy people (5% of 999)\nSo out of 51 positive results, only 1 is correct!\n\nThe actual probability is closer to 2%, not 95%. Mind blown yet?"
  },
  {
    "objectID": "posts/Information-Theory-In-ML/index.html#when-frequencies-fail",
    "href": "posts/Information-Theory-In-ML/index.html#when-frequencies-fail",
    "title": "Information Theory in Machine Learning: A Fun Approach",
    "section": "When Frequencies Fail",
    "text": "When Frequencies Fail\nBut here’s the real problem - we can’t actually run this experiment 1000 times for our current patient. We have:\n\nOne patient\nOne test result\nOne decision to make\n\nWe need a way to reason about this specific case, using everything we know."
  },
  {
    "objectID": "posts/Information-Theory-In-ML/index.html#enter-bayesian-thinking",
    "href": "posts/Information-Theory-In-ML/index.html#enter-bayesian-thinking",
    "title": "Information Theory in Machine Learning: A Fun Approach",
    "section": "Enter Bayesian Thinking",
    "text": "Enter Bayesian Thinking\nThis is where Bayesian probability shines. Instead of counting frequencies, we think about:\n\nWhat we knew before the test (prior)\nWhat the test tells us (evidence)\nHow to combine these (update)\n\nFor our patient:\n\nPrior: 1/1000 chance of disease (population rate)\nEvidence: Positive test result (95% accurate)\nUpdate: Combine these mathematically (we’ll show how!)"
  },
  {
    "objectID": "posts/Information-Theory-In-ML/index.html#the-bayesian-formula",
    "href": "posts/Information-Theory-In-ML/index.html#the-bayesian-formula",
    "title": "Information Theory in Machine Learning: A Fun Approach",
    "section": "The Bayesian Formula",
    "text": "The Bayesian Formula\nThis thinking gives us the famous Bayes’ Theorem: \\[\nP(Disease|Positive) = \\frac{P(Positive|Disease) \\times P(Disease)}{P(Positive)}\n\\]\nWhere:\n\n\\(P(Disease|Positive)\\) is what we want: probability of disease given a positive test\n\\(P(Positive|Disease)\\) is 0.95 (test sensitivity)\n\\(P(Disease)\\) is 1/1000 (disease prevalence)\n\\(P(Positive)\\) needs to be calculated using total probability\n\nTo find \\(P(Positive)\\), we consider both ways to get a positive result:\n\nHaving the disease and testing positive\nNot having the disease but testing positive anyway\n\nMathematically: \\(P(Positive) = P(Positive|Disease)P(Disease) + P(Positive|No Disease)P(No Disease)\\)\nPlugging in our numbers: \\(P(Positive) = 0.95 \\times (1/1000) + 0.05 \\times (999/1000)\\) = 0.0509\nNow we can solve our original equation:\n\\[\nP(Disease|Positive) = \\frac{0.95 \\times (1/1000)}{0.0509} \\approx 0.019\n\\]\nThat’s about 2%! Even with a “95% accurate” test, a positive result only means a 2% chance of actually having the disease. This surprising result comes from combining three pieces of information:\n\nHow rare the disease is (prior probability)\nHow accurate the test is (sensitivity)\nHow often it gives false alarms (specificity)\n\nModern ML models work in a similar way. When a model says “I’m 90% confident this is a cat’s image”, it’s not just matching pixels - it’s combining prior knowledge about what cats look like with the specific evidence in the image. Just like our medical test, it’s all about updating beliefs with evidence!"
  },
  {
    "objectID": "posts/The Transparent Algorithm/index.html#table-of-content",
    "href": "posts/The Transparent Algorithm/index.html#table-of-content",
    "title": "[WIP] The Transparent Algorithm",
    "section": "Table of Content",
    "text": "Table of Content\n\n\n\n#\nTitle\nDescription\n\n\n\n\n1\nThe Mathematics of Learning Guarantees\nFoundations of learning theory, error bounds, and the concept of misleading samples"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Swayam’s Scripts",
    "section": "",
    "text": "[WIP] The Transparent Algorithm\n\n\nA bridge between formal mathematical proofs and intuitive understanding, guiding readers through the theoretical foundations of machine learning with clarity and precision.\n\n\n\nML\n\n\nTheory\n\n\nBook\n\n\n\n\n\n\n\n\n\nMar 15, 2025\n\n\nSwayam Singh\n\n\n1 min\n\n\n35 words\n\n\n\n\n\n\n\n\n\n\n\n\nConcurrent C++ A Practical Guide\n\n\nA concise and practical guide to mastering C++ concurrency, covering threads, synchronization, parallelism, and debugging—all in one place.\n\n\n\nC++\n\n\nBook\n\n\n\n\n\n\n\n\n\nFeb 9, 2025\n\n\nSwayam Singh\n\n\n1 min\n\n\n5 words\n\n\n\n\n\n\n\n\n\n\n\n\nInformation Theory in Machine Learning: A Fun Approach\n\n\nTeaching Neural Networks to Handle Their Trust Issues\n\n\n\nML\n\n\nInformation-Theory\n\n\n\n\n\n\n\n\n\nJan 9, 2025\n\n\nSwayam Singh\n\n\n14 min\n\n\n2,650 words\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Perplexity\n\n\nA New Perspective on Model Uncertainty\n\n\n\nLLM\n\n\n\n\n\n\n\n\n\nOct 10, 2024\n\n\nSwayam Singh\n\n\n3 min\n\n\n595 words\n\n\n\n\n\n\n\n\n\n\n\n\nNumpy QuadDType: Quadruple Precision for Everyone\n\n\nQuad Precision for All: Simplifying High-Accuracy Computing with numpy_quaddtype\n\n\n\nNumPy\n\n\nOpen-Source\n\n\n\n\n\n\n\n\n\nSep 30, 2024\n\n\nSwayam Singh\n\n\n1 min\n\n\n5 words\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Attention Mimicking Gradient Descent\n\n\n\n\n\n\nNLP\n\n\nTransformers\n\n\n\n\n\n\n\n\n\nOct 14, 2023\n\n\nSwayam Singh\n\n\n9 min\n\n\n1,671 words\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/The Transparent Algorithm/chapter-1.html",
    "href": "pages/The Transparent Algorithm/chapter-1.html",
    "title": "Chapter 1: The Mathematics of Learning Guarantees",
    "section": "",
    "text": "When a machine learning algorithm examines data and produces a model, how can we be confident that this model will perform well on new, unseen examples? This fundamental question lies at the heart of learning theory. Rather than focusing on specific algorithms like neural networks or decision trees, learning theory provides a mathematical framework to understand when and why learning works.\nConsider a simple problem: we want to determine if an email is spam based on its content. We might train a classifier on thousands of labeled examples, but how do we know if it will work well on future emails? Intuitively, we expect that with enough diverse training examples, our classifier should generalize well to new data. But how many examples are “enough”? And how can we formally prove this intuition?\nLearning theory gives us tools to answer these questions with mathematical precision. Instead of vague assurances, we can derive exact bounds on how likely our algorithm is to succeed and how much data it needs. This chapter introduces the fundamental concepts needed to build such guarantees.\n\n\n\nTo reason precisely about learning, we need formal definitions. Let’s establish our notation:\n\nInput space \\(\\mathcal{X}\\): The set of all possible inputs (e.g., all possible emails)\nOutput space \\(\\mathcal{Y}\\): The set of all possible outputs (e.g., “spam” or “not spam”)\nTraining sample \\(S = ((x_1, y_1), (x_2, y_2), ..., (x_m, y_m))\\): A set of m labeled examples\n\\(S|_x = (x_1, x_2, ..., x_m)\\): Just the inputs from the training sample\nDistribution \\(\\mathcal{D}\\): The probability distribution from which examples are drawn\nLabeling function \\(f: \\mathcal{X} \\rightarrow \\mathcal{Y}\\): The true, underlying function we’re trying to learn\nHypothesis class \\(\\mathcal{H}\\): The set of possible prediction rules our algorithm can output\nHypothesis \\(h \\in \\mathcal{H}\\): A specific prediction rule (e.g., a specific classifier)\n\nGiven these definitions, we can formalize two crucial concepts:\n\nTrue error: The probability that a hypothesis makes an incorrect prediction on an example drawn from the true distribution: \\[L_{(D,f)}(h) = \\mathbb{P}_{x \\sim D}[h(x) \\neq f(x)]\\]\nTraining error: The fraction of mistakes a hypothesis makes on the training sample: \\[L_S(h) = \\frac{1}{m}\\sum_{i=1}^{m} \\mathbb{1}_{[h(x_i) \\neq y_i]}\\] where \\(\\mathbb{1}\\) is the indicator function that equals 1 when \\(h(x_i) \\neq y_i\\) and 0 otherwise.\n\nOur learning algorithm examines the training data \\(S\\) and outputs a hypothesis \\(h_S\\). We want \\(h_S\\) to have low true error, but we can only measure its training error. The key challenge in learning theory is understanding the relationship between these two errors.\nFor simplicity, we’ll initially work under the realizability assumption: there exists at least one hypothesis \\(h^* \\in \\mathcal{H}\\) with zero true error (\\(L_{(D,f)}(h^*) = 0\\)). This means our hypothesis class contains at least one perfect predictor.\n\n\n\nWhen does a learning algorithm fail? We define failure as producing a hypothesis with true error exceeding some acceptable threshold \\(\\epsilon\\):\n\\[L_{(D,f)}(h_S) &gt; \\epsilon\\]\nOur goal is to understand how likely this failure is when we train on \\(m\\) randomly drawn examples.\nTo approach this systematically, let’s define the set of “bad” hypotheses:\n\\[\\mathcal{H}_B = \\{h \\in \\mathcal{H} : L_{(D,f)}(h) &gt; \\epsilon\\}\\]\nThese are hypotheses that perform poorly on the true distribution. If our algorithm outputs such a hypothesis, it fails.\nBut here’s the key insight: under the realizability assumption, our algorithm will always output a hypothesis with zero training error. So failure can only happen if some “bad” hypothesis also happens to have zero training error on our sample.\nThis leads us to define the set of “misleading samples”:\n\\[M = \\{S|_x : \\exists h \\in \\mathcal{H}_B, L_S(h) = 0\\}\\]\nThese are training sets where at least one bad hypothesis looks perfect (has zero training error). Since our algorithm chooses a hypothesis with zero training error, if a bad hypothesis has zero training error, our algorithm might select it and fail.\nThe crucial relationship is: \\[\\{S|_x : L_{(D,f)}(h_S) &gt; \\epsilon\\} \\subseteq M\\]\nEvery sample that leads to algorithm failure must be a misleading sample. This insight allows us to bound the probability of failure by analyzing the probability of drawing a misleading sample.\n\n\n\nNow we can calculate the probability of drawing a misleading sample. First, we express \\(M\\) as a union:\n\\[M = \\bigcup_{h \\in \\mathcal{H}_B} \\{S|_x : L_S(h) = 0\\}\\]\nThis allows us to apply the union bound:\n\\[\\mathcal{D}^m(\\{S|_x : L_{(D,f)}(h_S) &gt; \\epsilon\\}) \\leq \\mathcal{D}^m(M) \\leq \\sum_{h \\in \\mathcal{H}_B} \\mathcal{D}^m(\\{S|_x : L_S(h) = 0\\})\\]\nNext, we calculate the probability that a specific bad hypothesis \\(h\\) has zero training error. Since the true error of \\(h\\) is greater than \\(\\epsilon\\), and our samples are drawn independently:\n\\[\\mathcal{D}^m(\\{S|_x : L_S(h) = 0\\}) = \\mathcal{D}^m(\\{S|_x : \\forall i, h(x_i) = f(x_i)\\}) = \\prod_{i=1}^m \\mathcal{D}(\\{x_i : h(x_i) = f(x_i)\\})\\]\nFor each individual sample, the probability of correct classification is:\n\\[\\mathcal{D}(\\{x_i : h(x_i) = f(x_i)\\}) = 1 - L_{(D,f)}(h) \\leq 1 - \\epsilon\\]\nTherefore: \\[\\mathcal{D}^m(\\{S|_x : L_S(h) = 0\\}) \\leq (1 - \\epsilon)^m\\]\nTo simplify our expressions, we use the standard inequality \\(1 - \\epsilon \\leq e^{-\\epsilon}\\), which gives us:\n\\[\\mathcal{D}^m(\\{S|_x : L_S(h) = 0\\}) \\leq (1 - \\epsilon)^m \\leq e^{-\\epsilon m}\\]\nCombining this with our union bound:\n\\[\\mathcal{D}^m(\\{S|_x : L_{(D,f)}(h_S) &gt; \\epsilon\\}) \\leq \\sum_{h \\in \\mathcal{H}_B} e^{-\\epsilon m} = |\\mathcal{H}_B| \\cdot e^{-\\epsilon m} \\leq |\\mathcal{H}| \\cdot e^{-\\epsilon m}\\]\nThis gives us our fundamental bound: the probability of failure is at most \\(|\\mathcal{H}| \\cdot e^{-\\epsilon m}\\).\n\n\n\nThe bound we’ve derived forms the foundation of Probably Approximately Correct (PAC) learning. Let’s understand what it means in practical terms.\nIf we want to ensure that our algorithm fails with probability at most \\(\\delta\\) (a confidence parameter), we need:\n\\[|\\mathcal{H}| \\cdot e^{-\\epsilon m} \\leq \\delta\\]\nSolving for the required sample size \\(m\\):\n\\[e^{-\\epsilon m} \\leq \\frac{\\delta}{|\\mathcal{H}|}\\]\nTaking logarithms:\n\\[-\\epsilon m \\leq \\ln\\left(\\frac{\\delta}{|\\mathcal{H}|}\\right) = \\ln(\\delta) - \\ln(|\\mathcal{H}|)\\]\nDividing by \\(-\\epsilon\\) and flipping the inequality:\n\\[m \\geq \\frac{\\ln(|\\mathcal{H}|) - \\ln(\\delta)}{\\epsilon} = \\frac{\\ln(|\\mathcal{H}|/\\delta)}{\\epsilon}\\]\nThis gives us the PAC learning guarantee: With probability at least \\(1-\\delta\\), a learning algorithm that outputs a hypothesis with zero training error will have true error at most \\(\\epsilon\\), provided the training sample size is at least:\n\\[m \\geq \\frac{\\ln(|\\mathcal{H}|/\\delta)}{\\epsilon}\\]\nThis is a fundamental result that tells us exactly how much data we need to learn effectively. It shows that the sample complexity: - Scales logarithmically with the size of the hypothesis class (\\(|\\mathcal{H}|\\)) - Scales logarithmically with the inverse of the confidence parameter (\\(1/\\delta\\)) - Scales linearly with the inverse of the error threshold (\\(1/\\epsilon\\))\nThis means that we can learn even from very large hypothesis classes with a reasonable amount of data, as long as we’re willing to accept a small probability of failure and a small error threshold.\n\nWe can make the similar interpretation from the experiment shown in the image.\n\n\n\nIn this chapter, we developed the fundamental mathematics that allow us to provide guarantees about learning algorithms. Rather than relying on intuition or empirical evidence alone, we now have precise tools to analyze when and why learning succeeds.\nLet’s review the key insights we’ve gained:\n\nWe formalized the learning problem through precise definitions of hypotheses, errors, and learning algorithms.\nWe identified that learning failure occurs when our algorithm outputs a hypothesis with high true error.\nWe discovered that such failure can only happen when our training sample is “misleading” - making a bad hypothesis look perfect.\nWe derived tight bounds on the probability of drawing misleading samples, leading to our fundamental inequality: \\[\\mathcal{D}^m(\\{S|_x : L_{(D,f)}(h_S) &gt; \\epsilon\\}) \\leq |\\mathcal{H}| \\cdot e^{-\\epsilon m}\\]\nWe established the sample complexity of learning: \\[m \\geq \\frac{\\ln(|\\mathcal{H}|/\\delta)}{\\epsilon}\\] This tells us exactly how many examples we need to ensure our learning algorithm succeeds with high probability.\n\nThese results provide a solid mathematical foundation for understanding learning guarantees, but they rely on several assumptions that limit their direct application to many real-world scenarios. Most notably, we’ve worked under the realizability assumption (that a perfect hypothesis exists in our class), and we’ve assumed that our hypothesis class is finite.\nIn the next chapter, we’ll formalize and extend these insights by developing the Probably Approximately Correct (PAC) learning model. This model provides a comprehensive framework for analyzing learnability across different problem domains and algorithm types. We’ll remove the restrictive assumptions of this chapter, explore how to handle infinite hypothesis classes, and investigate learning without the realizability assumption.\nBy building on the mathematical foundations established here, we’ll develop a more complete understanding of what makes learning possible and what fundamental limitations exist in machine learning. This will ultimately help us answer one of the most profound questions in the field: What is learnable and what is not?"
  },
  {
    "objectID": "pages/The Transparent Algorithm/chapter-1.html#introduction-to-learning-theory",
    "href": "pages/The Transparent Algorithm/chapter-1.html#introduction-to-learning-theory",
    "title": "Chapter 1: The Mathematics of Learning Guarantees",
    "section": "",
    "text": "When a machine learning algorithm examines data and produces a model, how can we be confident that this model will perform well on new, unseen examples? This fundamental question lies at the heart of learning theory. Rather than focusing on specific algorithms like neural networks or decision trees, learning theory provides a mathematical framework to understand when and why learning works.\nConsider a simple problem: we want to determine if an email is spam based on its content. We might train a classifier on thousands of labeled examples, but how do we know if it will work well on future emails? Intuitively, we expect that with enough diverse training examples, our classifier should generalize well to new data. But how many examples are “enough”? And how can we formally prove this intuition?\nLearning theory gives us tools to answer these questions with mathematical precision. Instead of vague assurances, we can derive exact bounds on how likely our algorithm is to succeed and how much data it needs. This chapter introduces the fundamental concepts needed to build such guarantees."
  },
  {
    "objectID": "pages/The Transparent Algorithm/chapter-1.html#key-notation-and-definitions",
    "href": "pages/The Transparent Algorithm/chapter-1.html#key-notation-and-definitions",
    "title": "Chapter 1: The Mathematics of Learning Guarantees",
    "section": "",
    "text": "To reason precisely about learning, we need formal definitions. Let’s establish our notation:\n\nInput space \\(\\mathcal{X}\\): The set of all possible inputs (e.g., all possible emails)\nOutput space \\(\\mathcal{Y}\\): The set of all possible outputs (e.g., “spam” or “not spam”)\nTraining sample \\(S = ((x_1, y_1), (x_2, y_2), ..., (x_m, y_m))\\): A set of m labeled examples\n\\(S|_x = (x_1, x_2, ..., x_m)\\): Just the inputs from the training sample\nDistribution \\(\\mathcal{D}\\): The probability distribution from which examples are drawn\nLabeling function \\(f: \\mathcal{X} \\rightarrow \\mathcal{Y}\\): The true, underlying function we’re trying to learn\nHypothesis class \\(\\mathcal{H}\\): The set of possible prediction rules our algorithm can output\nHypothesis \\(h \\in \\mathcal{H}\\): A specific prediction rule (e.g., a specific classifier)\n\nGiven these definitions, we can formalize two crucial concepts:\n\nTrue error: The probability that a hypothesis makes an incorrect prediction on an example drawn from the true distribution: \\[L_{(D,f)}(h) = \\mathbb{P}_{x \\sim D}[h(x) \\neq f(x)]\\]\nTraining error: The fraction of mistakes a hypothesis makes on the training sample: \\[L_S(h) = \\frac{1}{m}\\sum_{i=1}^{m} \\mathbb{1}_{[h(x_i) \\neq y_i]}\\] where \\(\\mathbb{1}\\) is the indicator function that equals 1 when \\(h(x_i) \\neq y_i\\) and 0 otherwise.\n\nOur learning algorithm examines the training data \\(S\\) and outputs a hypothesis \\(h_S\\). We want \\(h_S\\) to have low true error, but we can only measure its training error. The key challenge in learning theory is understanding the relationship between these two errors.\nFor simplicity, we’ll initially work under the realizability assumption: there exists at least one hypothesis \\(h^* \\in \\mathcal{H}\\) with zero true error (\\(L_{(D,f)}(h^*) = 0\\)). This means our hypothesis class contains at least one perfect predictor."
  },
  {
    "objectID": "pages/The Transparent Algorithm/chapter-1.html#understanding-failure-events",
    "href": "pages/The Transparent Algorithm/chapter-1.html#understanding-failure-events",
    "title": "Chapter 1: The Mathematics of Learning Guarantees",
    "section": "",
    "text": "When does a learning algorithm fail? We define failure as producing a hypothesis with true error exceeding some acceptable threshold \\(\\epsilon\\):\n\\[L_{(D,f)}(h_S) &gt; \\epsilon\\]\nOur goal is to understand how likely this failure is when we train on \\(m\\) randomly drawn examples.\nTo approach this systematically, let’s define the set of “bad” hypotheses:\n\\[\\mathcal{H}_B = \\{h \\in \\mathcal{H} : L_{(D,f)}(h) &gt; \\epsilon\\}\\]\nThese are hypotheses that perform poorly on the true distribution. If our algorithm outputs such a hypothesis, it fails.\nBut here’s the key insight: under the realizability assumption, our algorithm will always output a hypothesis with zero training error. So failure can only happen if some “bad” hypothesis also happens to have zero training error on our sample.\nThis leads us to define the set of “misleading samples”:\n\\[M = \\{S|_x : \\exists h \\in \\mathcal{H}_B, L_S(h) = 0\\}\\]\nThese are training sets where at least one bad hypothesis looks perfect (has zero training error). Since our algorithm chooses a hypothesis with zero training error, if a bad hypothesis has zero training error, our algorithm might select it and fail.\nThe crucial relationship is: \\[\\{S|_x : L_{(D,f)}(h_S) &gt; \\epsilon\\} \\subseteq M\\]\nEvery sample that leads to algorithm failure must be a misleading sample. This insight allows us to bound the probability of failure by analyzing the probability of drawing a misleading sample."
  },
  {
    "objectID": "pages/The Transparent Algorithm/chapter-1.html#probabilistic-error-bounds",
    "href": "pages/The Transparent Algorithm/chapter-1.html#probabilistic-error-bounds",
    "title": "Chapter 1: The Mathematics of Learning Guarantees",
    "section": "",
    "text": "Now we can calculate the probability of drawing a misleading sample. First, we express \\(M\\) as a union:\n\\[M = \\bigcup_{h \\in \\mathcal{H}_B} \\{S|_x : L_S(h) = 0\\}\\]\nThis allows us to apply the union bound:\n\\[\\mathcal{D}^m(\\{S|_x : L_{(D,f)}(h_S) &gt; \\epsilon\\}) \\leq \\mathcal{D}^m(M) \\leq \\sum_{h \\in \\mathcal{H}_B} \\mathcal{D}^m(\\{S|_x : L_S(h) = 0\\})\\]\nNext, we calculate the probability that a specific bad hypothesis \\(h\\) has zero training error. Since the true error of \\(h\\) is greater than \\(\\epsilon\\), and our samples are drawn independently:\n\\[\\mathcal{D}^m(\\{S|_x : L_S(h) = 0\\}) = \\mathcal{D}^m(\\{S|_x : \\forall i, h(x_i) = f(x_i)\\}) = \\prod_{i=1}^m \\mathcal{D}(\\{x_i : h(x_i) = f(x_i)\\})\\]\nFor each individual sample, the probability of correct classification is:\n\\[\\mathcal{D}(\\{x_i : h(x_i) = f(x_i)\\}) = 1 - L_{(D,f)}(h) \\leq 1 - \\epsilon\\]\nTherefore: \\[\\mathcal{D}^m(\\{S|_x : L_S(h) = 0\\}) \\leq (1 - \\epsilon)^m\\]\nTo simplify our expressions, we use the standard inequality \\(1 - \\epsilon \\leq e^{-\\epsilon}\\), which gives us:\n\\[\\mathcal{D}^m(\\{S|_x : L_S(h) = 0\\}) \\leq (1 - \\epsilon)^m \\leq e^{-\\epsilon m}\\]\nCombining this with our union bound:\n\\[\\mathcal{D}^m(\\{S|_x : L_{(D,f)}(h_S) &gt; \\epsilon\\}) \\leq \\sum_{h \\in \\mathcal{H}_B} e^{-\\epsilon m} = |\\mathcal{H}_B| \\cdot e^{-\\epsilon m} \\leq |\\mathcal{H}| \\cdot e^{-\\epsilon m}\\]\nThis gives us our fundamental bound: the probability of failure is at most \\(|\\mathcal{H}| \\cdot e^{-\\epsilon m}\\)."
  },
  {
    "objectID": "pages/The Transparent Algorithm/chapter-1.html#the-probably-approximately-correct-framework",
    "href": "pages/The Transparent Algorithm/chapter-1.html#the-probably-approximately-correct-framework",
    "title": "Chapter 1: The Mathematics of Learning Guarantees",
    "section": "",
    "text": "The bound we’ve derived forms the foundation of Probably Approximately Correct (PAC) learning. Let’s understand what it means in practical terms.\nIf we want to ensure that our algorithm fails with probability at most \\(\\delta\\) (a confidence parameter), we need:\n\\[|\\mathcal{H}| \\cdot e^{-\\epsilon m} \\leq \\delta\\]\nSolving for the required sample size \\(m\\):\n\\[e^{-\\epsilon m} \\leq \\frac{\\delta}{|\\mathcal{H}|}\\]\nTaking logarithms:\n\\[-\\epsilon m \\leq \\ln\\left(\\frac{\\delta}{|\\mathcal{H}|}\\right) = \\ln(\\delta) - \\ln(|\\mathcal{H}|)\\]\nDividing by \\(-\\epsilon\\) and flipping the inequality:\n\\[m \\geq \\frac{\\ln(|\\mathcal{H}|) - \\ln(\\delta)}{\\epsilon} = \\frac{\\ln(|\\mathcal{H}|/\\delta)}{\\epsilon}\\]\nThis gives us the PAC learning guarantee: With probability at least \\(1-\\delta\\), a learning algorithm that outputs a hypothesis with zero training error will have true error at most \\(\\epsilon\\), provided the training sample size is at least:\n\\[m \\geq \\frac{\\ln(|\\mathcal{H}|/\\delta)}{\\epsilon}\\]\nThis is a fundamental result that tells us exactly how much data we need to learn effectively. It shows that the sample complexity: - Scales logarithmically with the size of the hypothesis class (\\(|\\mathcal{H}|\\)) - Scales logarithmically with the inverse of the confidence parameter (\\(1/\\delta\\)) - Scales linearly with the inverse of the error threshold (\\(1/\\epsilon\\))\nThis means that we can learn even from very large hypothesis classes with a reasonable amount of data, as long as we’re willing to accept a small probability of failure and a small error threshold.\n\nWe can make the similar interpretation from the experiment shown in the image."
  },
  {
    "objectID": "pages/The Transparent Algorithm/chapter-1.html#summary-and-looking-ahead",
    "href": "pages/The Transparent Algorithm/chapter-1.html#summary-and-looking-ahead",
    "title": "Chapter 1: The Mathematics of Learning Guarantees",
    "section": "",
    "text": "In this chapter, we developed the fundamental mathematics that allow us to provide guarantees about learning algorithms. Rather than relying on intuition or empirical evidence alone, we now have precise tools to analyze when and why learning succeeds.\nLet’s review the key insights we’ve gained:\n\nWe formalized the learning problem through precise definitions of hypotheses, errors, and learning algorithms.\nWe identified that learning failure occurs when our algorithm outputs a hypothesis with high true error.\nWe discovered that such failure can only happen when our training sample is “misleading” - making a bad hypothesis look perfect.\nWe derived tight bounds on the probability of drawing misleading samples, leading to our fundamental inequality: \\[\\mathcal{D}^m(\\{S|_x : L_{(D,f)}(h_S) &gt; \\epsilon\\}) \\leq |\\mathcal{H}| \\cdot e^{-\\epsilon m}\\]\nWe established the sample complexity of learning: \\[m \\geq \\frac{\\ln(|\\mathcal{H}|/\\delta)}{\\epsilon}\\] This tells us exactly how many examples we need to ensure our learning algorithm succeeds with high probability.\n\nThese results provide a solid mathematical foundation for understanding learning guarantees, but they rely on several assumptions that limit their direct application to many real-world scenarios. Most notably, we’ve worked under the realizability assumption (that a perfect hypothesis exists in our class), and we’ve assumed that our hypothesis class is finite.\nIn the next chapter, we’ll formalize and extend these insights by developing the Probably Approximately Correct (PAC) learning model. This model provides a comprehensive framework for analyzing learnability across different problem domains and algorithm types. We’ll remove the restrictive assumptions of this chapter, explore how to handle infinite hypothesis classes, and investigate learning without the realizability assumption.\nBy building on the mathematical foundations established here, we’ll develop a more complete understanding of what makes learning possible and what fundamental limitations exist in machine learning. This will ultimately help us answer one of the most profound questions in the field: What is learnable and what is not?"
  },
  {
    "objectID": "posts/Self-Attention Mimicking Gradient Descent/index.html",
    "href": "posts/Self-Attention Mimicking Gradient Descent/index.html",
    "title": "Self-Attention Mimicking Gradient Descent",
    "section": "",
    "text": "Self-Attention Mimicking Gradient Descent\n\n\n\nThis section of paper Uncovering mesa-optimization algorithms in Transformers presents a theoretical construction where a linear self-attention layer in a Transformer architecture can mimic a single step of gradient descent for a linear regression task.\n\nToken Construction (from the paper):\n\n\nTokens: A set of tokens \\(E_T\\) is constructed with \\(T = N\\) such that \\(e_t = (y_{\\tau,i}, x_{\\tau,i})\\), where \\(y_{\\tau,i}\\) and \\(x_{\\tau,i}\\) are concatenated.\nQuery Token: A query token \\(e_{T+1}\\) is created as \\(e_{T+1} = (-W_0 x_{\\tau,\\text{test}}, x_{\\tau,\\text{test}})\\) . This token represents the test input for which a prediction is to be made.\n\n\nWhy doing \\(-W_0 x_{\\tau,\\text{test}}\\) ? \\(W_o\\) represent the model’s initial weights, multiplying it with \\(x_{\\tau,\\text{test}}\\) provides initial context for prediction (basically giving a perspective to start with). The \\(-_(ve)\\) sign is to align with the GD update, where we move in the direction opposite to the gradient. So we can say that The initial negative prediction in the query token provides a starting point, and the self-attention mechanism’s update to this prediction results in a new prediction that mimics\n\n\n\nConditions (from the paper):\n\n\nAll bias terms are zero\n\nbasically only using the model weights without any bias term\n\n\\[\nW^T_k W_q = \\begin{bmatrix} 0 & 0 \\\\ 0 & I_x \\end{bmatrix}\n\\]\n\nAs per my understanding, the relevance of this condition depends is as following\n\nSo, I’ll write the equations for calculating attention weights and they are self-explanatory to be honest (also this is my fav interpretation of this condition)\n\\[\ne = \\begin{bmatrix} y_1 \\\\ x_1 \\\\ x_2 \\end{bmatrix} \\\\\n\\text{ and }q = \\begin{bmatrix} q_y \\\\ q_{x1} \\\\ q_{x2} \\end{bmatrix}\n\\]\n\\[k = W_k e\\]\n\\[q' = W_q q\\]\n\\[\\text{attention weight} \\propto k^T q'\\]\nGiven the condition, this becomes: \\[\\text{weight} \\propto e^T W^T_k W_q q\\]\nNow, plugging in the condition\n\\[W^T_k W_q = \\begin{bmatrix} 0 & 0 \\\\ 0 & I_x \\end{bmatrix}\\]\nthe interaction simplifies to: \\[\n\\text{weight} \\propto \\begin{bmatrix} y_1 & x_1 & x_2 \\end{bmatrix} \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 1 & 0\\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} q_y \\\\ q_{x1} \\\\ q_{x2} \\end{bmatrix}\n\\] \\[\n\\text{weight} \\propto \\begin{bmatrix} 0 & x_1 & x_2 \\end{bmatrix} \\begin{bmatrix} q_y \\\\ q_{x1} \\\\ q_{x2} \\end{bmatrix}\n\\] \\[\\text{weight} \\propto x_1 q_{x1} + x_2 q_{x2}\\]\nSo what we can see here is that, the attention-weights are proportional to the dot-product of x-component of inputs and query (rejecting the influence of y-component)\n\n\\[ P W_v = \\begin{bmatrix} -\\eta I_y & \\eta W_0 \\\\ 0 & 0 \\end{bmatrix}\\]\n\nThis is the another condition and lets try to understand what it means, here \\(\\eta\\) is the learning-rate, \\(*P*\\) is a projection matrix, and \\(*W_v*\\) is the weight matrix for the “values”.\nBefore diving into the interpretation of this condition, we should know that the Linear Self Attention can be represented as \\(P V_{t} K^T_{t} q_{t}\\) (considering 1 head for simplicity) here \\(P\\) is just the Projection matrix and the rest of the term is exact same as calculating attention.\nThe matrix product \\(PW_v\\) determines how the values contribute to the updated query token. Now lets try to understand it\nIn this interpretation, I’ll again try to resolve this by solving equations, since our new value_matrix is \\(PW_v\\) i.e we can calculate values of input token as\n\\[v = PW_ve\\]\nwhere \\(e = \\begin{bmatrix} y_1 \\\\ x_1 \\\\ x_2 \\end{bmatrix}\\) and \\(P W_v = \\begin{bmatrix} -\\eta & \\eta W_{01} & \\eta W_{02} \\\\ 0 & 0 & 0\\\\ 0 & 0 & 0\\end{bmatrix}\\)\n\\[v = -\\eta y + \\eta W_{01}x_1 +\\eta W_{02}x_2\\]\nBased on this equation we can interpret the condition as:\n\nThe Upper Left Block \\((-\\eta I_y)\\): This block scales the y-component of the values (the outputs) by −η. In the context of gradient descent, the update is proportional to the negative gradient. This block captures the idea that the update to our model’s prediction should be in the opposite direction of the error (difference between prediction and actual output). Multiplying by −η ensures that if our model’s prediction is too high, it gets adjusted downwards, and if it’s too low, it gets adjusted upwards.\nThe Upper Right Block (\\(\\eta W_0\\)): This block scales the x-component of the values (the inputs) and then multiplies by the initial weight \\(*W_0\\).* This captures the contribution of the inputs to the gradient of the loss with respect to the model parameters. In other words, it represents how much each input feature contributes to the error. Multiplying by \\(*W_0*\\) gives the model’s initial reliance on each feature, and the entire product indicates how the model should adjust its reliance on each feature to minimize the error.\nThe Lower Blocks (0 matrices): These blocks ensure that the x-component of the updated query token remains unchanged. This is consistent with the idea that the input part of our test example doesn’t change; only our model’s prediction (or representation) of it does.\n\n\nThat covers all the required conditions, now let’s see that satisfying these conditions, how the weights of the self-attention will contain the gradient of the loss of a Linear Regression objective.\n\n\nGradient Descent in Linear Regression:\n\nFor a linear regression task, the gradient descent update rule is: \\[\\Delta W_0 = \\eta \\sum_{i=1}^{N} (y_{\\tau,i} - W_0 x_{\\tau,i}) x^T_{\\tau,i}\\]\n\n\nLinear Self-Attention Layer:\n\nGiven the conditions, the self-attention mechanism computes the weighted sum of values based on the similarity (dot product) of the query with the keys.\nTo prove: The weights in the attention mechanism will effectively compute the gradient of the loss with respect to the model parameters.\nLet’s move to the final derivation:\n\nAttention Weights Calculation: Given the second condition, the attention weights are determined by the dot product of the query with the keys: \\[\\alpha_t = q \\cdot k_t\\] Where \\(q\\) is the query, and \\(k_t\\) is the key for the t-th token.\nValue Update: The update to the query token using the attention mechanism is: \\[e_{T+1,\\text{new}} = \\sum_{t=1}^{T} \\alpha_t v_t\\] Where \\(v_t\\) is the value for the t-th token.\n\nGiven the third condition, the value for each token is: \\[v_t = P W_v e_t\\] Substituting this in, we get: \\[e_{T+1,\\text{new}} = \\sum_{t=1}^{T} \\alpha_t P W_v e_t\\]\nMatching with Gradient Descent: Given the attention weights \\((\\alpha_t)\\) and the gradient descent update rule, the y-component of the updated query token is: \\[\ne_{T+1,\\text{new,y}} = \\sum_{t=1}^{T} \\alpha_t (-\\eta y_{\\tau,i} + \\eta W_0 x_{\\tau,i}) \\\\\ne_{T+1,\\text{new,y}} = \\sum_{t=1}^{T} \\alpha_t \\eta (W_0 x_{\\tau,i} - y_{\\tau,i})\n\\]\nNow, comparing this with the gradient descent update rule, we can see that the term inside the summation \\((W_0x_τ,_iy_τ,_i)\\) is essentially the error in prediction for the training data.\nGiven that the attention weights \\(α_t\\) effectively compute a weighted version of this error, the entire equation can be seen as a weighted sum of errors, which is analogous to the gradient in gradient descent.\n\n\nApproximation:\n\nNow, for the test input \\(x_{\\tau,\\text{test}}\\), the prediction using the initial weights \\(W_0\\) is \\(W_0 x_{\\tau,\\text{test}}\\). After one step of gradient descent, the prediction becomes: \\[(W_0 - \\Delta W_0) x_{\\tau,\\text{test}}\\]\nGiven that the self-attention mechanism’s update to the query token is designed to mimic one step of gradient descent, we can approximate: \\[e_{T+1,\\text{new,y}} \\approx (W_0 - \\Delta W_0) x_{\\tau,\\text{test}}\\]\nThis approximation captures the essence of the theoretical construction: the self-attention mechanism updates the query token in a way that mimics the behavior of gradient descent\nAlso the rearrangement and approximation are based on the insight that the self-attention mechanism’s update to the query token aligns with the gradient descent update rule for linear regression, allowing the mechanism to make predictions consistent with gradient descent optimization."
  },
  {
    "objectID": "posts/Understanding Perplexity/index.html",
    "href": "posts/Understanding Perplexity/index.html",
    "title": "Understanding Perplexity",
    "section": "",
    "text": "Recently, I was reading the Chapter 5 (Pretraining) of the book “Build a Large Language Model (From Scratch)” by Sebastian Raschka. I stumbled upon an intriguing interpretation of perplexity. The author noted:\n\n“Perplexity is often considered more interpretable than the raw loss value because it signifies the effective vocabulary size about which the model is uncertain at each step.”\n\nIn simple words, If for some model the perplexity comes out to be \\(N\\) then it means that the model is \\(N\\) tokens uncertain about the correct next-token, it is considering all the \\(N\\) tokens as the potential candidate for the output token.\nThis statement resonated with me, as I had always viewed perplexity as just a performance metric. I began to wonder: can we mathematically derive this interpretation? Does the underlying math support this idea?\nLet’s delve into the equations and explore how perplexity relates to the model’s uncertainty about the next token in a sequence.\n\n\nIn language modeling, cross-entropy loss is a critical metric that helps us evaluate how well a model predicts the next token in a sequence. For a sequence of tokens \\(x = (x_1, x_2, ..., x_T)\\), the cross-entropy loss is calculated as:\n\\[\n\\mathcal{L} = - \\frac{1}{T} \\sum_{t=1}^{T} \\log P(x_t | \\mathbf{x}_{&lt;t})\n\\]\nwhere:\n\n\\(T\\) is the total number of tokens in the sequence.\n\n\n\\(P(x_t | \\mathbf{x}_{&lt;t})\\) is the predicted probability of the actual token \\(x_t\\) given the preceding context \\(\\mathbf{x}_{&lt;t}\\).\n\n This formulation averages the negative log-likelihood across all tokens, providing a measure of how well the model’s predictions align with the true tokens.\n\n\n\nPerplexity serves as a complementary metric to cross-entropy loss and is defined as the exponentiation of the loss:\n\\[\n\\text{Perplexity} = \\exp(\\mathcal{L})\n\\]\nThis formulation provides a more interpretable value, as it represents the effective number of choices the model considers when predicting the next token. A lower perplexity indicates higher confidence in predictions, while a higher perplexity signifies greater uncertainty.\n\n\n\n\n\n\nNote\n\n\n\nBefore going into maths, lets understand one thing\nIntuitively, for a completely uncertain model, selection for some next-token can be any from the whole vocabulary with each token having same probability of being the next token\n\n\n\n\n\nTo understand the interpretation of perplexity in terms of effective vocabulary size, let’s consider an extreme case where the model is completely uncertain about the next token. In this scenario, the model assigns equal probability to every token in the vocabulary of size \\(V\\). Thus, the probability of each token can be expressed as:\n\\[\nP(x_t | \\mathbf{x}_{&lt;t}) = \\frac{1}{V}\n\\]\nNow, substituting this uniform probability into the cross-entropy loss equation, we get:\n\\[\n\\mathcal{L} = - \\frac{1}{T} \\sum_{t=1}^{T} \\log P(x_t | \\mathbf{x}_{&lt;t}) = - \\log \\frac{1}{V} = \\log V\n\\]\nHere, \\(-\\log P(x_t | \\mathbf{x}_{&lt;t})\\) reflects the loss incurred for each token when the model is entirely uncertain.\n\n\n\nNext, we can use the perplexity formula to analyze this situation:\n\\[\n\\text{Perplexity} = \\exp(\\mathcal{L}) = \\exp(\\log V) = V\n\\]\nThis result reveals a fascinating insight: when the model is completely uncertain, the perplexity is exactly equal to the size of the vocabulary \\(V\\).\n\n\nNow, what does this mean in terms of interpretation? When the perplexity equals \\(V\\), it indicates that the model is effectively considering all \\(V\\) tokens as potential candidates for the next token, reflecting a state of maximum uncertainty.\nOn the other hand, if the model has a lower perplexity, say 100, it means that the model behaves as if it is uncertain only among 100 tokens. This aligns perfectly with the statement from Raschka’s book: perplexity signifies the effective vocabulary size about which the model is uncertain at each step."
  },
  {
    "objectID": "posts/Understanding Perplexity/index.html#cross-entropy-loss-a-quick-recap",
    "href": "posts/Understanding Perplexity/index.html#cross-entropy-loss-a-quick-recap",
    "title": "Understanding Perplexity",
    "section": "",
    "text": "In language modeling, cross-entropy loss is a critical metric that helps us evaluate how well a model predicts the next token in a sequence. For a sequence of tokens \\(x = (x_1, x_2, ..., x_T)\\), the cross-entropy loss is calculated as:\n\\[\n\\mathcal{L} = - \\frac{1}{T} \\sum_{t=1}^{T} \\log P(x_t | \\mathbf{x}_{&lt;t})\n\\]\nwhere:\n\n\\(T\\) is the total number of tokens in the sequence.\n\n\n\\(P(x_t | \\mathbf{x}_{&lt;t})\\) is the predicted probability of the actual token \\(x_t\\) given the preceding context \\(\\mathbf{x}_{&lt;t}\\).\n\n This formulation averages the negative log-likelihood across all tokens, providing a measure of how well the model’s predictions align with the true tokens."
  },
  {
    "objectID": "posts/Understanding Perplexity/index.html#defining-perplexity",
    "href": "posts/Understanding Perplexity/index.html#defining-perplexity",
    "title": "Understanding Perplexity",
    "section": "",
    "text": "Perplexity serves as a complementary metric to cross-entropy loss and is defined as the exponentiation of the loss:\n\\[\n\\text{Perplexity} = \\exp(\\mathcal{L})\n\\]\nThis formulation provides a more interpretable value, as it represents the effective number of choices the model considers when predicting the next token. A lower perplexity indicates higher confidence in predictions, while a higher perplexity signifies greater uncertainty.\n\n\n\n\n\n\nNote\n\n\n\nBefore going into maths, lets understand one thing\nIntuitively, for a completely uncertain model, selection for some next-token can be any from the whole vocabulary with each token having same probability of being the next token"
  },
  {
    "objectID": "posts/Understanding Perplexity/index.html#analyzing-the-uniform-distribution-case",
    "href": "posts/Understanding Perplexity/index.html#analyzing-the-uniform-distribution-case",
    "title": "Understanding Perplexity",
    "section": "",
    "text": "To understand the interpretation of perplexity in terms of effective vocabulary size, let’s consider an extreme case where the model is completely uncertain about the next token. In this scenario, the model assigns equal probability to every token in the vocabulary of size \\(V\\). Thus, the probability of each token can be expressed as:\n\\[\nP(x_t | \\mathbf{x}_{&lt;t}) = \\frac{1}{V}\n\\]\nNow, substituting this uniform probability into the cross-entropy loss equation, we get:\n\\[\n\\mathcal{L} = - \\frac{1}{T} \\sum_{t=1}^{T} \\log P(x_t | \\mathbf{x}_{&lt;t}) = - \\log \\frac{1}{V} = \\log V\n\\]\nHere, \\(-\\log P(x_t | \\mathbf{x}_{&lt;t})\\) reflects the loss incurred for each token when the model is entirely uncertain."
  },
  {
    "objectID": "posts/Understanding Perplexity/index.html#connecting-loss-and-perplexity",
    "href": "posts/Understanding Perplexity/index.html#connecting-loss-and-perplexity",
    "title": "Understanding Perplexity",
    "section": "",
    "text": "Next, we can use the perplexity formula to analyze this situation:\n\\[\n\\text{Perplexity} = \\exp(\\mathcal{L}) = \\exp(\\log V) = V\n\\]\nThis result reveals a fascinating insight: when the model is completely uncertain, the perplexity is exactly equal to the size of the vocabulary \\(V\\).\n\n\nNow, what does this mean in terms of interpretation? When the perplexity equals \\(V\\), it indicates that the model is effectively considering all \\(V\\) tokens as potential candidates for the next token, reflecting a state of maximum uncertainty.\nOn the other hand, if the model has a lower perplexity, say 100, it means that the model behaves as if it is uncertain only among 100 tokens. This aligns perfectly with the statement from Raschka’s book: perplexity signifies the effective vocabulary size about which the model is uncertain at each step."
  }
]